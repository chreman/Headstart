"doi","id","subject","title","authors","year","publisher","resulttype","language","journal","url","paper_abstract","project_id","accessright","x","y","area_uri","cluster_labels","area","citation_count","cited_by_tweeters_count","readers.mendeley","readers","file_hash"
"","ec_fp7_ict__::f948ef2dc329ce0c64a8cdbc69fea12d","large hierarchical;phoneme recognition;recognition large","Phoneme recognition with large hierarchical reservoirs","#N/D","2010-01-01","","publication","","","","","231267","Closed Access","0.5223","0.36","1","Acoustic modeling, Acoustic models, Based acoustic","Acoustic modeling, Acoustic models, Based acoustic",NA,NA,NA,"",""
"","dedup_wf_001::70fee0868b57d2433e5621b91ff24292","System dynamics","Role of the frontal cortex in solving the exploration-exploitation trade-off","Khamassi, Mehdi","2010-08-06","HAL CCSD","publication","","","https://hal.archives-ouvertes.fr/hal-00553443/document","While many electrophysiological recordings and computational modeling work have investigated the role of the frontal cortex in reinforcement learning (learning by trial-and-error to adapt action values), it is not yet clear how the brain flexibly regulates in a task-appropriate way crucial parameters of learning such as the learning rate and the exploration rate. In a previous work, we proposed a computational model based on the meta-learning theoretical framework where the frontal cortex extracts feedback signals 1) to update action values based on a reward prediction error, 2) to estimate the level of exploration based on the current reward average, 3) to select action based on this exploration rate. This model helped us draw a set of experimental predictions. Here we show a model-based analysis of single-unit recordings in the monkey prefrontal cortex so as to test these predictions. We found neural subpopulations activities consistent with these three functions. We also found global properties of the recorded neural ensemble - such as variations in spatial selectivity - which were predicted by our model. Such an approach, gathering computational modeling and neurophysiology, can help understand complex activities of neural ensembles related to decision making.","231267","Open Access","-0.1225","-0.4658","6","Computational model, Reinforcement learning, Task monitoring","Computational model, Reinforcement learning, Task monitoring",NA,NA,NA,"",""
"","dedup_wf_001::80355a0235e3fb3162388c6e4185a130","reward","A Computational Model of Integration between Reinforcement Learning and Task Monitoring in the Prefrontal Cortex : Reinforcement Learning, Task Monitoring and the Prefrontal Cortex","Khamassi, Mehdi","2010-08-24","HAL CCSD","publication","","","http://www.hal.inserm.fr/inserm-00548868/document","International audience; Taking inspiration from neural principles of decision-makingis of particular interest to help improve adaptivity of artificial systems.Research at the crossroads of neuroscience and artificial intelligence in thelast decade has helped understanding how the brain organizes reinforcementlearning (RL) processes (the adaptation of decisions based on feedbackfrom the environment). The current challenge is now to understandhow the brain flexibly regulates parameters of RL such as the explorationrate based on the task structure, which is called meta-learning ([1]: Doya,2002). Here, we propose a computational mechanism of exploration regulationbased on real neurophysiological and behavioral data recorded inmonkey prefrontal cortex during a visuo-motor task involving a clear distinctionbetween exploratory and exploitative actions. We first fit trialby-trial choices made by the monkeys with an analytical reinforcementlearning model. We find that the model which has the highest likelihoodof predicting monkeys' choices reveals different exploration rates at differenttask phases. In addition, the optimized model has a very high learningrate, and a reset of action values associated to a cue used in the task tosignal condition changes. Beyond classical RL mechanisms, these resultssuggest that the monkey brain extracted task regularities to tune learningparameters in a task-appropriate way. We finally use these principles todevelop a neural network model extending a previous cortico-striatal loopmodel. In our prefrontal cortex component, prediction error signals are extractedto produce feedback categorization signals. The latter are used toboost exploration after errors, and to attenuate it during exploitation, ensuringa lock on the currently rewarded choice. This model performs thetask like monkeys, and provides a set of experimental predictions to betested by future neurophysiological recordings.","231267","Open Access","0.0194","-0.5879","6","Computational model, Reinforcement learning, Task monitoring","Computational model, Reinforcement learning, Task monitoring",NA,NA,NA,"",""
"","webcrawl____::018aba5af4421b546b38e6406c30d6d5","architectures large;large scale;learning architectures","Oger: Modular Learning Architectures For Large-Scale Sequential Processing","Verstraeten, David","2012-01-01","MICROTOME PUBL","publication","","JOURNAL OF MACHINE LEARNING RESEARCH","","","231267","Open Access","-0.5349","-0.5902","12","Architectures large, Large scale, Learning architectures","Architectures large, Large scale, Learning architectures",NA,NA,NA,"",""
"","od_______330::1b3be891629ee95fefa558db641864c6","Technology and Engineering","Connected digit recognition by means of Reservoir Computing","Jalalvand, Azarakhsh","2011-01-01","International Speech Communication Association (ISCA)","publication","","","","Most automatic speech recognition systems employ Hidden Markov Models with Gaussian mixture emission distributions to model the acoustics. There have been several attempts however to challenge this approach, e.g. by introducing a neural network (NN) as an alternative acoustic model. Although the performance of these so-called hybrid systems is actually quite good, their training is often problematic and time consuming.
			By using a reservoir – this is a recurrent NN with only the output weights being trainable – we can overcome this disadvantage and yet obtain good accuracy. In this paper, we propose the first reservoir-based connected digit recognition system, and we demonstrate good performance on the Aurora-2 testbed. Since RC is a new technology, we anticipate that our present system is still sub-optimal, and further improvements are possible.","231267","Restricted","0.4603","-0.0432","7","Digit recognition, Technology and engineering, Reservoir computing","Digit recognition, Technology and engineering, Reservoir computing",NA,NA,NA,"",""
"","dedup_wf_001::b690447180a3d6534243121e9bbd3a42","Cortical treatments","A three-layered cortical network encodes identity and abstract categorical structure of behavioral sequences as in the primate lateral prefrontal cortex","Hinaut, Xavier","2010-08-06","HAL CCSD","publication","","","https://hal.archives-ouvertes.fr/hal-00553435/document","Categorical encoding is crucial for mastering large bodies of related sensory experiences. Recent single-unit recording studies in the macaque prefrontal cortex have demonstrated two characteristic forms of neural encoding of the sequential structure of the animal's behaviour. One population of neurons encodes the specific behavioural sequences. A second population of neurons encodes the sequence category (e.g. ABAB, AABB or AAAA) and does not differentiate sequences within the category [1]. Interestingly these neurons are intermingled in the lateral prefrontal cortex, and not topographically segregated. Here we report on a neural network simulation study that reproduces and explains these results. We simulate a cortical circuit as three 5x5 layers (infra-granular, granular, and supra-granular) of leaky integrator neurons with a sigmoidal output function, and we examine 103 such circuits running in parallel. The model is presented with 11 4-element sequences following Shima et al. We isolated one subpopulation of neurons each of whose activity predicts individual sequences, and a second population that predicts category independent of the specific sequence. We argue that a richly interconnected cortical circuit is capable of internally generating a neural representation of category membership, thus significantly extending the scope of recurrent network computation.","231267","Open Access","0.2139","-0.6317","5","Abstract categorical structure, Behavioral sequences, Encodes identity","Abstract categorical structure, Behavioral sequences, Encodes identity",NA,NA,NA,"",""
"","od_______330::c8e28da3c1cddf199da55358d7a19054","Technology and Engineering","Phoneme recognition with large hierarchical reservoirs","Triefenbach, Fabian","2010-01-01","Neural Information Processing System Foundation","publication","","","","Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.","231267","Restricted","0.5364","0.1715","7","Digit recognition, Technology and engineering, Reservoir computing","Digit recognition, Technology and engineering, Reservoir computing",NA,NA,NA,"",""
"","od_______330::5b951d1db3bc5880e78f6277b043a623","Reservoir Computing","Continuous digit recognition in noise: reservoirs can do an excellent job!","Jalalvand, Azarakhsh","2012-01-01","International Speech Communication Association (ISCA)","publication","","","","In this paper a formerly proposed continuous digit recognition system based on Reservoir Computing (RC) is improved in two respects: (1)the single reservoir is substituted by a stack of reservoirs, and (2)the straightforward mapping of reservoir outputs to state likelihoods is replaced by a trained non-parametric mapping. Furthermore, it is shown that a reservoir-based method can improve a model trained on clean speech to work better in a noisy condition from which it has a number of unknown digit string recordings available. The first two improvements have lead to a system that outperforms a HMM-based system with the same noise robust features as input. The model adaptation offers a promising supplementary gain at modest noise levels.","231267","Restricted","0.378","0.0772","7","Digit recognition, Technology and engineering, Reservoir computing","Digit recognition, Technology and engineering, Reservoir computing",NA,NA,NA,"",""
"10.1007/978-3-642-15193-4_40","ec_fp7_ict__::3be4ba8b63cd5147eba2c6953b1ac8fa"," computational;computational model;integration reinforcement","A computational model of integration between reinforcement learning and task monitoring in the prefrontal cortex","Khamassi M.","2010-01-01","","publication","","","","","231267","Closed Access","-0.0057","-0.3932","6","Computational model, Reinforcement learning, Task monitoring","Computational model, Reinforcement learning, Task monitoring",0,NA,NA,"",""
"10.1007/s00422-012-0516-4","dedup_wf_001::3018767c791d1728d627507d5bcf1e90","Department of Informatics","The role of feedback in morphological computation with compliant bodies","Hauser, Helmut","2012-01-01","Springer","publication","","","http://www.zora.uzh.ch/id/eprint/72784/1/Hauser_et_al_Role_of_feedback.pdf","The generation of robust periodic movements of complex nonlinear robotic systems is inherently difficult, especially, if parts of the robots are compliant. It has previously been proposed that complex nonlinear features of a robot, similarly as in biological organisms, might possibly facilitate its control. This bold hypothesis, commonly referred to as morphological computation, has recently received some theoretical support by Hauser et al. (Biol Cybern 105:355–370, doi:10.1007/s00422-012-0471-0, 2012). We show in this article that this theoretical support can be extended to cover not only the case of fading memory responses to external signals, but also the essential case of autonomous generation of adaptive periodic patterns, as, e.g., needed for locomotion. The theory predicts that feedback into the morphological computing system is necessary and sufficient for such tasks, for which a fading memory is insufficient. We demonstrate the viability of this theoretical analysis through computer simulations of complex nonlinear mass–spring systems that are trained to generate a large diversity of periodic movements by adapting the weights of a simple linear feedback device. Hence, the results of this article substantially enlarge the theoretically tractable application domain of morphological computation in robotics, and also provide new paradigms for understanding control principles of biological organisms.","248311","Open Access","-0.4296","0.25","4","Autonomous goal, Goal oriented, Internal models","Autonomous goal, Goal oriented, Internal models",24,"2","99","",""
"10.1007/s11063-013-9279-8","dedup_wf_001::9130a570854f9bcca69cefbc33f77503","Computationally efficient","Optimized Parameter Search for Large Datasets of the Regularization Parameter and Feature Selection for Ridge Regression","Buteneers, Pieter","2013-01-01","SPRINGER","publication","","NEURAL PROCESSING LETTERS","","In this paper we propose mathematical optimizations to select the optimal regularization parameter for ridge regression using cross-validation. The resulting algorithm is suited for large datasets and the computational cost does not depend on the size of the training set. We extend this algorithm to forward or backward feature selection in which the optimal regularization parameter is selected for each possible feature set. These feature selection algorithms yield solutions with a sparse weight matrix using a quadratic cost on the norm of the weights. A naive approach to optimizing the ridge regression parameter has a computational complexity of the order with the number of applied regularization parameters, the number of folds in the validation set, the number of input features and the number of data samples in the training set. Our implementation has a computational complexity of the order . This computational cost is smaller than that of regression without regularization for large datasets and is independent of the number of applied regularization parameters and the size of the training set. Combined with a feature selection algorithm the algorithm is of complexity and for forward and backward feature selection respectively, with the number of selected features and the number of removed features. This is an order faster than and for the naive implementation, with for large datasets. To show the performance and reduction in computational cost, we apply this technique to train recurrent neural networks using the reservoir computing approach, windowed ridge regression, least-squares support vector machines (LS-SVMs) in primal space using the fixed-size LS-SVM approximation and extreme learning machines.","231267","Open Access","-0.5203","-0.3551","4","Autonomous goal, Goal oriented, Internal models","Autonomous goal, Goal oriented, Internal models",7,NA,NA,"",""
"10.1016/j.jphysparis.2011.07.010","webcrawl____::1a36766f8dbb2ce1d4523763a977c37d","abstract categorical;behavioral sequences;categorical structure","A three-layered model of primate prefrontal cortex encodes identity and abstract categorical structure of behavioral sequences","Hinaut, Xavier","2011-01-01","ELSEVIER SCI LTD","publication","","JOURNAL OF PHYSIOLOGY-PARIS","","","231267","Closed Access","0.3706","-0.6625","5","Abstract categorical structure, Behavioral sequences, Encodes identity","Abstract categorical structure, Behavioral sequences, Encodes identity",2,NA,NA,"",""
"10.1016/j.neunet.2009.08.008","dedup_wf_001::5acc45dff0e45632f639a959a96c807e","Reservoir computing","Memory in linear recurrent neural networks in continuous time","Hermans, Michiel","2010-01-01","PERGAMON-ELSEVIER SCIENCE LTD","publication","","NEURAL NETWORKS","","","231267","Closed Access","-0.0207","0.2377","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",16,NA,NA,"",""
"10.1016/j.neunet.2011.08.004","dedup_wf_001::5a973eec7f74156af08f3719b3bd6567",": Computer science [C05] [Engineering, computing & technology]","Learning slow features with reservoir computing for biologically-inspired robot localization","Antonelo, Eric","2012-01-01","PERGAMON-ELSEVIER SCIENCE LTD","publication","","NEURAL NETWORKS","","","231267","Open Access","-0.3047","0.0378","4","Autonomous goal, Goal oriented, Internal models","Autonomous goal, Goal oriented, Internal models",7,NA,NA,"",""
"10.1038/nn.3038","dedup_wf_001::6351551d635aca41b1c9c441b0cf2672","genetic structures","Theta coupling between V4 and prefrontal cortex predicts visual short-term memory performance","Liebe S.,","2012-03-20","NATURE PUBLISHING GROUP","publication","","","","Short-term memory requires communication between multiple brain regions that collectively mediate the encoding and maintenance of sensory information. It has been suggested that oscillatory synchronization underlies intercortical communication. Yet, whether and how distant cortical areas cooperate during visual memory remains elusive. We examined neural interactions between visual area V4 and the lateral prefrontal cortex using simultaneous local field potential (LFP) recordings and single-unit activity (SUA) in monkeys performing a visual short-term memory task. During the memory period, we observed enhanced between-area phase synchronization in theta frequencies (3–9 Hz) of LFPs together with elevated phase locking of SUA to theta oscillations across regions. In addition, we found that the strength of intercortical locking was predictive of the animals’ behavioral performance. This suggests that theta-band synchronization coordinates action potential communication between V4 and prefrontal cortex that may contribute to the maintenance of visual short-term memories.","231267","Open Access","0.3796","0.555","8","Short term memory, Visual short term","Short term memory, Visual short term",97,"19","422","",""
"10.1093/cercor/bhs348","dedup_wf_001::a3c1a5c70788ce0c3c2b6db07caffafc","chaotic neural;complex computational;computational structures","Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning","Hoerzer, Gregor M.","2014-01-01","OXFORD UNIV PRESS INC","publication","","CEREBRAL CORTEX","","","248311","Closed Access","-0.5155","-0.1785","13","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural",36,"26","241","",""
"10.1109/ici.2011.50","dedup_wf_001::19aa8a1cee22f96214b26b8da31ae728","Technology and Engineering","Can non-linear readout nodes enhance the performance of reservoir-based speech recognizers?","Triefenbach, Fabian","2011-01-01","IEEE Computer Society","publication","","","","","231267","Open Access","0.7736","0.1487","11","Technology and engineering","Technology and engineering",4,NA,NA,"",""
"10.1109/ijcnn.2010.5596884","dedup_wf_001::636b54ea79e15f4cb7bc8628f515002e","Mathematics and Statistics","Memory in reservoirs for high dimensional input","Hermans, Michiel","2010-01-01","IEEE","publication","","","","Reservoir Computing (RC) is a recently introduced scheme to employ recurrent neural networks while circumventing the difficulties that typically appear when training the recurrent weights. The ‘reservoir’ is a fixed randomly initiated recurrent network which receives input via a random mapping. Only an instantaneous linear mapping from the network to the output is trained which can be done with linear regression. In this paper we study dynamical properties of reservoirs receiving a high number of inputs. More specifically, we investigate how the internal state of the network retains fading memory of its input signal. Memory properties for random recurrent networks have been thoroughly examined in past research, but only for one-dimensional input. Here we take into account statistics which will typically occur in high dimensional signals. We find useful empirical data which expresses how memory in recurrent networks is distributed over the individual principal components of the input.","231267","Open Access","0.151","0.2462","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",3,NA,NA,"",""
"10.1109/iros.2010.5652697","ec_fp7_ict__::5c051605b9a4b658716c83866f273a92","cooperative human;human robot;independent cooperative","Towards a platform-independent cooperative human-robot interaction system: I. Perception","Lallee S.","2010-01-01","","publication","","","","","231267","Closed Access","-0.2813","0.6809","10","Cooperative human, Emergent pattern, Human robot","Cooperative human, Emergent pattern, Human robot",10,NA,NA,"",""
"10.1109/iscas.2010.5537568","dedup_wf_001::140da78a1af688308d7fd6fc18506df1","speech recognition","One step backpropagation through time for learning input mapping in reservoir computing applied to speech recognition","Hermans, Michiel","2010-01-01","IEEE","publication","","","","Recurrent neural networks are very powerful engines for processing information that is coded in time, however, many problems with common training algorithms, such as Backpropagation Through Time, remain. Because of this, another important learning setup known as Reservoir Computing has appeared in recent years, where one uses an essentially untrained network to perform computations. Though very successful in many applications, using a random network can be quite inefficient when considering the required number of neurons and the associated computational costs. In this paper we introduce a highly simplified version of Backpropagation Through Time by basically truncating the error backpropagation to one step back in time, and we combine this with the classic Reservoir Computing setup using an instantaneous linear readout. We apply this setup to a spoken digit recognition task and show it to give very good results for small networks.","231267","Open Access","0.0644","0.0381","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",6,NA,NA,"",""
"10.1109/lsp.2014.2302080","webcrawl____::f7108253a06674b38cb3097a8f8cc907","acoustic models;based acoustic;continuous speech","Large Vocabulary Continuous Speech Recognition With Reservoir-Based Acoustic Models","Triefenbach, Fabian","2014-01-01","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","publication","","IEEE SIGNAL PROCESSING LETTERS","","","231267","Closed Access","0.7136","-0.0984","1","Acoustic modeling, Acoustic models, Based acoustic","Acoustic modeling, Acoustic models, Based acoustic",4,NA,NA,"",""
"10.1109/robot.2010.5509212","ec_fp7_ict__::9283bbfd76964aae562a738e69cb0b1f","autonomous goal;goal oriented;internal models","Supervised learning of internal models for autonomous goal-oriented robot navigation using reservoir computing","Antonelo E.A.","2010-01-01","","publication","","","","","231267","Closed Access","-0.1698","-0.0844","4","Autonomous goal, Goal oriented, Internal models","Autonomous goal, Goal oriented, Internal models",7,NA,NA,"",""
"10.1109/tasl.2013.2280209","webcrawl____::95a9785aee862f3b8f7fdbec72acf9e0","acoustic modeling;modeling hierarchical","Acoustic Modeling With Hierarchical Reservoirs","Triefenbach, Fabian","2013-01-01","IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","publication","","IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING","","","231267","Closed Access","0.644","-0.2115","1","Acoustic modeling, Acoustic models, Based acoustic","Acoustic modeling, Acoustic models, Based acoustic",19,NA,NA,"",""
"10.1152/jn.00845.2009","dedup_wf_001::1522b075e55e11e3e8326612dcc613ec","Articles","Statistical Comparison of Spike Responses to Natural Stimuli in Monkey Area V1 With Simulated Responses of a Detailed Laminar Network Model for a Patch of V1","Rasch, Malte J.","2010-11-24","American Physiological Society","publication","","JOURNAL OF NEUROPHYSIOLOGY","","A major goal of computational neuroscience is the creation of computer models for cortical areas whose response to sensory stimuli resembles that of cortical areas in vivo in important aspects. It is seldom considered whether the simulated spiking activity is realistic (in a statistical sense) in response to natural stimuli. Because certain statistical properties of spike responses were suggested to facilitate computations in the cortex, acquiring a realistic firing regimen in cortical network models might be a prerequisite for analyzing their computational functions. We present a characterization and comparison of the statistical response properties of the primary visual cortex (V1) in vivo and in silico in response to natural stimuli. We recorded from multiple electrodes in area V1 of 4 macaque monkeys and developed a large state-of-the-art network model for a 5 &#215; 5-mm patch of V1 composed of 35,000 neurons and 3.9 million synapses that integrates previously published anatomical and physiological details. By quantitative comparison of the model response to the &#8220;statistical fingerprint&#8221; of responses in vivo, we find that our model for a patch of V1 responds to the same movie in a way which matches the statistical structure of the recorded data surprisingly well. The deviation between the firing regimen of the model and the in vivo data are on the same level as deviations among monkeys and sessions. This suggests that, despite strong simplifications and abstractions of cortical network models, they are nevertheless capable of generating realistic spiking activity. To reach a realistic firing state, it was not only necessary to include both N-methyl-d-aspartate and GABAB synaptic conductances in our model, but also to markedly increase the strength of excitatory synapses onto inhibitory neurons (&gt;2-fold) in comparison to literature values, hinting at the importance to carefully adjust the effect of inhibition for achieving realistic dynamics in current network models.","231267","Open Access","0.3128","-0.4283","3","Articles, Broad range, Comparison of spike","Articles, Broad range, Comparison of spike",13,NA,"107","",""
"10.1152/jn.00935.2011","webcrawl____::095bbd526cc0466b6adf79c55fb0f8dd","analysis information;a neurons;encoded spikes","A quantitative analysis of information about past and present stimuli encoded by spikes of A1 neurons","Klampfl, Stefan","2012-01-01","AMER PHYSIOLOGICAL SOC","publication","","JOURNAL OF NEUROPHYSIOLOGY","","","231267","Closed Access","0.2582","0.721","14","Analog neurons, Information bottleneck","Analog neurons, Information bottleneck",15,"2","85","",""
"10.1162/neco.2009.01-09-947","dedup_wf_001::d20062dd0b69899eeb35c25de438f351","analog neurons;binary analog;computing binary","Connectivity, Dynamics, and Memory in Reservoir Computing with Binary and Analog Neurons","Buesing, Lars","2010-01-01","MIT PRESS","publication","","NEURAL COMPUTATION","","","231267","Closed Access","0.096","0.4566","14","Analog neurons, Information bottleneck","Analog neurons, Information bottleneck",59,NA,NA,"",""
"10.1162/neco.2010.03-09-980","dedup_wf_001::1b672367efab642eb0209eeb2c873efc","decision making;learning decision","Reward-modulated Hebbian learning of decision making","Pfeiffer M.","2010-01-01","M I T PRESS","publication","","Neural Computation","","","231267","Closed Access","-0.7051","-0.1619","13","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural",15,NA,NA,"",""
"10.1162/neco.2010.07-09-1070","dedup_wf_001::c4fb98fe3c7e90b1473eca066dfce873","CURRENTS","Fast and Exact Simulation Methods Applied on a Broad Range of Neuron Models","D'Haene, Michiel","2010-01-01","M I T PRESS","publication","","NEURAL COMPUTATION","","Recently, van Elberg and van Ooyen (2009) published a generalization of the Event-Based Integration Scheme for an Integrate-and-Fire neuron model with exponentially decaying excitatory currents and double exponential inhibitory synaptic currents, introduced by Carnevale and Hines. In the paper, it was shown that the constraints on the synaptic time constants imposed by the Newton-Raphson iteration scheme, can be relaxed. In this note, we show that according to the results published in D'Haene et al. (2009), a further generalization is possible eliminating any constraint on the time constants. We also demonstrate that in fact a wide range of linear neuron models can be efficiently simulated with this computation scheme, including neuron models mimicking complex neuronal behavior. These results can change the way complex neuronal spiking behavior is modeled: instead of highly non linear neuron models with few state variables, it is possible to efficiently simulate linear models with a large number of state variables.","231267","Restricted","0.4523","-0.3835","3","Articles, Broad range, Comparison of spike","Articles, Broad range, Comparison of spike",4,NA,NA,"",""
"10.1162/neco.2010.08-09-1084","dedup_wf_001::4ed5baf214aa8bdc2fee5de61368f394","information bottleneck;neuron information; spiking","A Spiking Neuron as Information Bottleneck","Buesing, Lars","2010-01-01","M I T PRESS","publication","","NEURAL COMPUTATION","","","231267","Closed Access","0.0673","0.7127","14","Analog neurons, Information bottleneck","Analog neurons, Information bottleneck",8,NA,NA,"",""
"10.1162/neco_a_00050","dedup_wf_001::d07d39dafe363c00ff6cc313adf8bff6","basis emergent;discrimination neural;emergent pattern","A theoretical basis for emergent pattern discrimination in neural systems through slow feature extraction","Klampfl S.","2010-01-01","MIT PRESS","publication","","Neural Computation","","","231267","Closed Access","-0.3928","0.5201","10","Cooperative human, Emergent pattern, Human robot","Cooperative human, Emergent pattern, Human robot",8,NA,NA,"",""
"10.1371/journal.pcbi.1004967","dedup_wf_001::dc77da091ef30d994df4d0c4caf1e9f0","Research Article","Reservoir Computing Properties of Neural Dynamics in Prefrontal Cortex.","Pierre Enel","2016-06-01","Public Library of Science (PLoS)","publication","","PLoS Computational Biology","","Primates display a remarkable ability to adapt to novel situations. Determining what is most pertinent in these situations is not always possible based only on the current sensory inputs, and often also depends on recent inputs and behavioral outputs that contribute to internal states. Thus, one can ask how cortical dynamics generate representations of these complex situations. It has been observed that mixed selectivity in cortical neurons contributes to represent diverse situations defined by a combination of the current stimuli, and that mixed selectivity is readily obtained in randomly connected recurrent networks. In this context, these reservoir networks reproduce the highly recurrent nature of local cortical connectivity. Recombining present and past inputs, random recurrent networks from the reservoir computing framework generate mixed selectivity which provides pre-coded representations of an essentially universal set of contexts. These representations can then be selectively amplified through learning to solve the task at hand. We thus explored their representational power and dynamical properties after training a reservoir to perform a complex cognitive task initially developed for monkeys. The reservoir model inherently displayed a dynamic form of mixed selectivity, key to the representation of the behavioral context over time. The pre-coded representation of context was amplified by training a feedback neuron to explicitly represent this context, thereby reproducing the effect of learning and allowing the model to perform more robustly. This second version of the model demonstrates how a hybrid dynamical regime combining spatio-temporal processing of reservoirs, and input driven attracting dynamics generated by the feedback neuron, can be used to solve a complex cognitive task. We compared reservoir activity to neural activity of dorsal anterior cingulate cortex of monkeys which revealed similar network dynamics. We argue that reservoir computing is a pertinent framework to model local cortical dynamics and their contribution to higher cognitive function.","270490","Open Access","0.1344","-0.1785","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",13,"50","113","",""
"10.1371/journal.pone.0052946","dedup_wf_001::f853b2532e67d03d8982e8b5735faea8","Robot","Depuis une mémoire autobiographique synthétique à l'émergence de différents niveaux de soi : étude de cas chez le robot humanoïde iCub","Pointeau, Grégoire","2015-10-06","HAL CCSD","publication","","","https://tel.archives-ouvertes.fr/tel-01269702/document","The objective of the following study is to define the influence of an Autobiographical Memory and its two main components : the Episodic Memory and the Semantic Memory in the emergence of the notion of “self”. I will focus on the functional part of the autobiographical memory rather that on its technical and neuronal aspect. Then I will describe the implementation of an synthetic autobiographical memory in an Humanoid Robot : the iCub. I will show how this synthetic autobiographical memory can participate with the help of several reasoning modules, in the emergence of the self. Concerning the aspect of self, we decided to work about Ulric Neisser’s four components of the self described in 1995 : the Ecological Self, the Interpersonal Self, the Conceptual Self and the Temporally Extended Self; L'objectif de l'étude suivante est de déterminer l'influence de la mémoire autobiographique et de ses deux principaux composants : la mémoire épisodique et la mémoire sémantique dans l'émergence de la notion de soi. Je vais me concentrer sur la composante fonctionnelle de la mémoire autobiographique davantage que sur ses aspects anatomiques et neuronaux. Je vais ensuite décrire l'implémentation d'une mémoire autobiographique synthétique chez un robot humanoïde : l'iCub. Puis, je vais montrer comment cette mémoire synthétique peut participer avec l'aide de procédés de raisonnement, `a l'émergence d'un “soi”. Concernant cet aspect du soi, nous avons décidé de concentrer notre travail sur la définition d'Ulric Neisser de 1995 de quatre composants du soi : le “Soi Ecologique”, le “Soi Interpersonnel”, le “Soi Conceptuel” et le “Soi Etendu dans le Temps”","270490","Open Access","-0.6028","0.4975","15","Robot","Robot",30,"7","80","",""
"10.1371/journal.pone.0052946","dedup_wf_001::f853b2532e67d03d8982e8b5735faea8","Robot","Depuis une mémoire autobiographique synthétique à l'émergence de différents niveaux de soi : étude de cas chez le robot humanoïde iCub","Pointeau, Grégoire","2015-10-06","HAL CCSD","publication","","","https://tel.archives-ouvertes.fr/tel-01269702/document","The objective of the following study is to define the influence of an Autobiographical Memory and its two main components : the Episodic Memory and the Semantic Memory in the emergence of the notion of “self”. I will focus on the functional part of the autobiographical memory rather that on its technical and neuronal aspect. Then I will describe the implementation of an synthetic autobiographical memory in an Humanoid Robot : the iCub. I will show how this synthetic autobiographical memory can participate with the help of several reasoning modules, in the emergence of the self. Concerning the aspect of self, we decided to work about Ulric Neisser’s four components of the self described in 1995 : the Ecological Self, the Interpersonal Self, the Conceptual Self and the Temporally Extended Self; L'objectif de l'étude suivante est de déterminer l'influence de la mémoire autobiographique et de ses deux principaux composants : la mémoire épisodique et la mémoire sémantique dans l'émergence de la notion de soi. Je vais me concentrer sur la composante fonctionnelle de la mémoire autobiographique davantage que sur ses aspects anatomiques et neuronaux. Je vais ensuite décrire l'implémentation d'une mémoire autobiographique synthétique chez un robot humanoïde : l'iCub. Puis, je vais montrer comment cette mémoire synthétique peut participer avec l'aide de procédés de raisonnement, `a l'émergence d'un “soi”. Concernant cet aspect du soi, nous avons décidé de concentrer notre travail sur la définition d'Ulric Neisser de 1995 de quatre composants du soi : le “Soi Ecologique”, le “Soi Interpersonnel”, le “Soi Conceptuel” et le “Soi Etendu dans le Temps”","270490","Open Access","-0.6028","0.4975","15","Robot","Robot",30,"7","80","",""
"10.1371/journal.pone.0052946","dedup_wf_001::dbccedf46caa4bdee737f0d82bc1091d","Computational Biology","Real-time parallel processing of grammatical structure in the fronto-striatal system: a recurrent network simulation study using reservoir computing.","Xavier Hinaut","","Public Library of Science (PLoS)","publication","","PLoS ONE","","Sentence processing takes place in real-time. Previous words in the sentence can influence the processing of the current word in the timescale of hundreds of milliseconds. Recent neurophysiological studies in humans suggest that the fronto-striatal system (frontal cortex, and striatum ? the major input locus of the basal ganglia) plays a crucial role in this process. The current research provides a possible explanation of how certain aspects of this real-time processing can occur, based on the dynamics of recurrent cortical networks, and plasticity in the cortico-striatal system. We simulate prefrontal area BA47 as a recurrent network that receives on-line input about word categories during sentence processing, with plastic connections between cortex and striatum. We exploit the homology between the cortico-striatal system and reservoir computing, where recurrent frontal cortical networks are the reservoir, and plastic cortico-striatal synapses are the readout. The system is trained on sentence-meaning pairs, where meaning is coded as activation in the striatum corresponding to the roles that different nouns and verbs play in the sentences. The model learns an extended set of grammatical constructions, and demonstrates the ability to generalize to novel constructions. It demonstrates how early in the sentence, a parallel set of predictions are made concerning the meaning, which are then confirmed or updated as the processing of the input sentence proceeds. It demonstrates how on-line responses to words are influenced by previous words in the sentence, and by previous sentences in the discourse, providing new insight into the neurophysiology of the P600 ERP scalp response to grammatical complexity. This demonstrates that a recurrent neural network can decode grammatical structure from sentences in real-time in order to generate a predictive representation of the meaning of the sentences. This can provide insight into the underlying mechanisms of human cortico-striatal function in sentence processing.","270490","Open Access","-0.1867","-0.6202","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",30,"7","80","",""
"10.1371/journal.pone.0052946","dedup_wf_001::dbccedf46caa4bdee737f0d82bc1091d","Computational Biology","Real-time parallel processing of grammatical structure in the fronto-striatal system: a recurrent network simulation study using reservoir computing.","Xavier Hinaut","","Public Library of Science (PLoS)","publication","","PLoS ONE","","Sentence processing takes place in real-time. Previous words in the sentence can influence the processing of the current word in the timescale of hundreds of milliseconds. Recent neurophysiological studies in humans suggest that the fronto-striatal system (frontal cortex, and striatum ? the major input locus of the basal ganglia) plays a crucial role in this process. The current research provides a possible explanation of how certain aspects of this real-time processing can occur, based on the dynamics of recurrent cortical networks, and plasticity in the cortico-striatal system. We simulate prefrontal area BA47 as a recurrent network that receives on-line input about word categories during sentence processing, with plastic connections between cortex and striatum. We exploit the homology between the cortico-striatal system and reservoir computing, where recurrent frontal cortical networks are the reservoir, and plastic cortico-striatal synapses are the readout. The system is trained on sentence-meaning pairs, where meaning is coded as activation in the striatum corresponding to the roles that different nouns and verbs play in the sentences. The model learns an extended set of grammatical constructions, and demonstrates the ability to generalize to novel constructions. It demonstrates how early in the sentence, a parallel set of predictions are made concerning the meaning, which are then confirmed or updated as the processing of the input sentence proceeds. It demonstrates how on-line responses to words are influenced by previous words in the sentence, and by previous sentences in the discourse, providing new insight into the neurophysiology of the P600 ERP scalp response to grammatical complexity. This demonstrates that a recurrent neural network can decode grammatical structure from sentences in real-time in order to generate a predictive representation of the meaning of the sentences. This can provide insight into the underlying mechanisms of human cortico-striatal function in sentence processing.","270490","Open Access","-0.1867","-0.6202","2","Reservoir computing, Speech recognition","Reservoir computing, Speech recognition",30,"7","80","",""
"10.1523/jneurosci.4284-09.2010","dedup_wf_001::06010e68e1c8361bc5dda92a12fce964","Quantitative Biology::Neurons and Cognition","A reward-modulated Hebbian learning rule can explain experimentally observed network reorganization in a brain control task","Legenstein R.","2010-01-01","SOC NEUROSCIENCE","publication","","Journal of Neuroscience","","It has recently been shown in a brain-computer interface experiment that motor cortical neurons change their tuning properties selectively to compensate for errors induced by displaced decoding parameters. In particular, it was shown that the 3D tuning curves of neurons whose decoding parameters were re-assigned changed more than those of neurons whose decoding parameters had not been re-assigned. In this article, we propose a simple learning rule that can reproduce this effect. Our learning rule uses Hebbian weight updates driven by a global reward signal and neuronal noise. In contrast to most previously proposed learning rules, this approach does not require extrinsic information to separate noise from signal. The learning rule is able to optimize the performance of a model system within biologically realistic periods of time under high noise levels. Furthermore, when the model parameters are matched to data recorded during the brain-computer interface learning experiments described above, the model produces learning effects strikingly similar to those found in the experiments.","1R01NS050256-01A2","Open Access","-0.3211","-0.4163","13","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural","Modulated hebbian learning, Reward modulated hebbian, Chaotic neural",51,NA,NA,"",""
"10.3389/fnbot.2010.00008","dedup_wf_001::beeb83f0c4016889252eb38ea3cf0106","action perception","Linking language with embodied and teleological representations of action for humanoid cognition","Stephane Lallee","2010-06-01","Frontiers Media S.A.","publication","","Frontiers in Neurorobotics","","The current research extends our framework for embodied language and action comprehension to include a teleological representation that allows goal-based reasoning for novel actions.  The objective of this work is to implement and demonstrate the advantages of a hybrid, embodied-teleological approach to action-language interaction, both from a theoretical perspective, and via results from human-robot interaction experiments with the iCub robot.  We first demonstrate how a framework for embodied language comprehension allows the system to develop a baseline set of representations for processing goal-directed actions such as &ldquo;take&rdquo;, &ldquo;cover&rdquo;, and &ldquo;give&rdquo;.  Spoken language and visual perception are input modes for these representations, and the generation of spoken language is the output mode.  Moving towards a teleological (goal-based reasoning) approach, a crucial component of the new system is the representation of the subcomponents of these actions, which includes relations between initial enabling states, and final resulting states for these actions.  We demonstrate how grammatical categories including causal connectives (e.g. because, if-then) can allow spoken language to enrich the learned set of state-action-state (SAS) representations.  We then examine how this enriched SAS inventory enhances the robot&rsquo;s ability to represent perceived actions in which the environment inhibits goal achievement.  The paper addresses how language comes to reflect the structure of action, and how it can subsequently be used as an input and output vector for embodied and teleological aspects of action.","231267","Open Access","-0.7058","0.1884","9","Acquisition and production, Action for humanoid, Action perception","Acquisition and production, Action for humanoid, Action perception",8,NA,"51","",""
"10.3389/fnbot.2014.00016","dedup_wf_001::a9d7699cd1a5014769fddd61c9c7c45d","anytime processing","Exploring the acquisition and production of grammatical constructions through human-robot interaction with echo state networks","Hinaut, Xavier","2014-05-01","Frontiers Media S.A.","publication","","Frontiers in Neurorobotics","","One of the principal functions of human language is to allow people to coordinate joint action. This includes the description of events, requests for action, and their organization in time. A crucial component of language acquisition is learning the grammatical structures that allow the expression of such complex meaning related to physical events. The current research investigates the learning of grammatical constructions and their temporal organization in the context of human-robot physical interaction with the embodied sensorimotor humanoid platform, the iCub. We demonstrate three noteworthy phenomena. First, a recurrent network model is used in conjunction with this robotic platform to learn the mappings between grammatical forms and predicate-argument representations of meanings related to events, and the robot's execution of these events in time. Second, this learning mechanism functions in the inverse sense, i.e., in a language production mode, where rather than executing commanded actions, the robot will describe the results of human generated actions. Finally, we collect data from naïve subjects who interact with the robot via spoken language, and demonstrate significant learning and generalization results. This allows us to conclude that such a neural language learning system not only helps to characterize and understand some aspects of human language acquisition, but also that it can be useful in adaptive human-robot interaction.","612139","Open Access","-0.6191","0.0406","9","Acquisition and production, Action for humanoid, Action perception","Acquisition and production, Action for humanoid, Action perception",14,"1","25","",""
"10.3389/fncom.2010.00014","dedup_wf_001::218b1d76151a96a184eafcde2af4a02c","partial directed coherence","Directed coupling in local field potentials of macaque V4 during visual short-term memory revealed by multivariate autoregressive models","Hoerzer, Gregor M.","2010-01-01","FRONTIERS RES FOUND","publication","","FRONTIERS IN COMPUTATIONAL NEUROSCIENCE","","
			Processing and storage of sensory information is based on the interaction between different neural populations rather than the isolated activity of single neurons. In order to characterize the dynamic interaction and transient cooperation of sub-circuits within a neural network, multivariate autoregressive (MVAR) models have proven to be an important analysis tool. In this study, we apply directed functional coupling based on MVAR models and describe the temporal and spatial changes of functional coupling between simultaneously recorded local field potentials in extrastriate area V4 during visual memory. Specifically, we compare the strength and directional relations of coupling based on generalized partial directed coherence (GPDC) measures while two rhesus monkeys perform a visual short-term memory task. In both monkeys we find increases in theta power during the memory period that are accompanied by changes in directed coupling. These interactions are most prominent in the low frequency range encompassing the theta band (3–12 Hz) and, more importantly, are asymmetric between pairs of recording sites. Furthermore, we find that the degree of interaction decreases as a function of distance between electrode positions, suggesting that these interactions are a predominantly local phenomenon. Taken together, our results show that directed coupling measures based on MVAR models are able to provide important insights into the spatial and temporal formation of local functionally coupled ensembles during visual memory in V4. Moreover, our findings suggest that visual memory is accompanied not only by a temporary increase of oscillatory activity in the theta band, but by a direction-dependent change in theta coupling, which ultimately represents a change in functional connectivity within the neural circuit.

			","231267","Open Access","-0.1098","0.5509","8","Short term memory, Visual short term","Short term memory, Visual short term",6,NA,NA,"",""
