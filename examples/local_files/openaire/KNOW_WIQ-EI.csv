"id","subject","title","authors","year","publisher","resulttype","language","journal","url","paper_abstract","doi","x","y","area_uri","cluster_labels","area","readers","file_hash"
"dedup_wf_001::0808f12a8b20e6fc893d6f450792b24a","Irony detection","From humor recognition to irony detection: The figurative language of social media","Reyes, Antonio","2012","ELSEVIER SCIENCE BV","","","DATA & KNOWLEDGE ENGINEERING","","The research described in this paper is focused on analyzing two playful domains of language: humor and irony, in order to identify key values components for their automatic processing. In particular, we are focused on describing a model for recognizing these phenomena in social media, such as \"tweets\". Our experiments are centered on five data sets retrieved from Twitter taking advantage of user-generated tags, such as \"#humor\" and \"#irony\". The model, which is based on textual features, is assessed on two dimensions: representativeness and relevance. The results, apart from providing some valuable insights into the creative and figurative usages of language, are positive regarding humor, and encouraging regarding irony. (C) 2012 Elsevier B.V. All rights reserved.
			This work has been done in the framework of the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems and it has been partially funded by the European Commission as part of the WIQEI IRSES project (grant no. 269180) within the FP 7 Marie Curie People Framework, and by MICINN as part of the Text-Enterprise 2.0 project (TIN2009-13391-C04-03) within the Plan I + D + I. The National Council for Science and Technology (CONACyT - Mexico) has funded the research work of Antonio Reyes.
			Reyes Pérez, A.; Rosso, P.; Buscaldi, D. (2012). From humor recognition to Irony detection: The figurative language of social media. Data and Knowledge Engineering. 74:1-12. doi:10.1016/j.datak.2012.02.005
			Senia
			1
			12
			74","10.1016/j.datak.2012.02.005","-0.4326","0.1408","1","Automatic irony detection, Detecting irony","Automatic irony detection, Detecting irony","",""
"dedup_wf_001::08ba257a022547aad547e9249ae7cc04","Cross-Lingual Similarity","A comparison of approaches for measuring cross-lingual similarity of wikipedia articles","Barrón-Cedeño, Alberto","2014","Springer","","","","","Wikipedia has been used as a source of comparable texts
			for a range of tasks, such as Statistical Machine Translation and CrossLanguage
			Information Retrieval. Articles written in different languages
			on the same topic are often connected through inter-language-links. However,
			the extent to which these articles are similar is highly variable and
			this may impact on the use of Wikipedia as a comparable resource. In this
			paper we compare various language-independent methods for measuring
			cross-lingual similarity: character n-grams, cognateness, word count ratio,
			and an approach based on outlinks. These approaches are compared
			against a baseline utilising MT resources. Measures are also compared
			to human judgements of similarity using a manually created resource
			containing 700 pairs of Wikipedia articles (in 7 language pairs). Results
			indicate that a combination of language-independent models (char-ngrams,
			outlinks and word-count ratio) is highly effective for identifying
			cross-lingual similarity and performs comparably to language-dependent
			models (translation and monolingual analysis).
			Tacardi research project [TIN2012-38523-C02-00]
			DIANA-Applications [TIN2012-38603-C02- 01]
			WIQ-EI IRSES
			Barrón Cedeño, LA.; Paramita, ML.; Clough, P.; Rosso, P. (2014). A Comparison of Approaches for Measuring Cross-Lingual Similarity of Wikipedia Articles. En Advances in Information Retrieval. Springer Verlag (Germany). 424-429. doi:10.1007/978-3-319-06028-6_36
			Senia
			424
			429","10.1007/978-3-319-06028-6_36","0.6333","-0.2614","10","Approaches for measuring, Comparison of approaches, Conceptual thesaurus","Approaches for measuring, Comparison of approaches, Conceptual thesaurus","",""
"dedup_wf_001::46d4151a6189012d7c8c585221cefdfe","Conditional random field","Towards a Protein-Protein Interaction information extraction system: recognizing named entities","Danger Mercaderes, Roxana María","2014","Elsevier","","","","","The majority of biological functions of any living being are related to Protein Protein Interactions (PPI). PPI discoveries are reported in form of research publications whose volume grows day after day. Consequently, automatic PPI information extraction systems are a pressing need for biologists. In this paper we are mainly concerned with the named entity detection module of PPIES (the PPI information extraction system we are implementing) which recognizes twelve entity types relevant in PPI context. It is composed of two sub-modules: a dictionary look-up with extensive normalization and acronym detection, and a Conditional Random Field classifier. The dictionary look-up module has been tested with Interaction Method Task (IMT), and it improves by approximately 10% the current solutions that do not use Machine Learning (ML). The second module has been used to create a classifier using the Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA 04) data set. It does not use any external resources, or complex or ad hoc post-processing, and obtains 77.25%, 75.04% and 76.13 for precision, recall, and F1-measure, respectively, improving all previous results obtained for this data set.
			This work has been funded by MICINN, Spain, as part of the \"Juan de la Cierva\" Program and the Project DIANA-Applications (TIN2012-38603-C02-01), as well as the by the European Commission as part of the WIQ-EI IRSES Project (Grant No. 269180) within the FP 7 Marie Curie People Framework.
			Danger Mercaderes, RM.; Pla Santamaría, F.; Molina Marco, A.; Rosso, P. (2014). Towards a Protein-Protein Interaction information extraction system: recognizing named entities. Knowledge-Based Systems. 57:104-118. doi:10.1016/j.knosys.2013.12.010
			Senia
			104
			118
			57","10.1016/j.knosys.2013.12.010","-0.128","-0.6414","2","Arabic wordnet, Conditional random field","Arabic wordnet, Conditional random field","",""
"dedup_wf_001::5bf6b4763b0c3ddb33fdecf4da0f87e9","Short-text corpora","An efficient Particle Swarm Optimization approach to cluster short texts","Cagnina, Leticia","2014","ELSEVIER SCIENCE INC","","","INFORMATION SCIENCES","","This is the author’s version of a work that was accepted for publication in Information Sciencies. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Information Sciences, VOL 265, MAY 1 2014 DOI 10.1016/j.ins.2013.12.010.
			Short texts such as evaluations of commercial products, news, FAQ's and scientific abstracts are important resources on the Web due to the constant requirements of people to use this on line information in real life. In this context, the clustering of short texts is a significant analysis task and a discrete Particle Swarm Optimization (PSO) algorithm named CLUDIPSO has recently shown a promising performance in this type of problems. CLUDIPSO obtained high quality results with small corpora although, with larger corpora, a significant deterioration of performance was observed. This article presents CLUDIPSO*, an improved version of CLUDIPSO, which includes a different representation of particles, a more efficient evaluation of the function to be optimized and some modifications in the mutation operator. Experimental results with corpora containing scientific abstracts, news and short legal documents obtained from the Web, show that CLUDIPSO* is an effective clustering method for short-text corpora of small and medium size. (C) 2013 Elsevier Inc. All rights reserved.
			The research work is partially funded by the European Commission as part of the WIQ-EI IRSES research project (Grant No. 269180) within the FP 7 Marie Curie People Framework and it has been developed in the framework of the Microcluster VLC/Campus (International Campus of Excellence) on Multimodal Intelligent Systems. The research work of the first author is partially funded by the program PAID-02-10 2257 (Universitat Politecnica de Valencia) and CONICET (Argentina).
			Cagnina, L.; Errecalde, M.; Ingaramo, D.; Rosso, P. (2014). An efficient Particle Swarm Optimization approach to cluster short texts. Information Sciences. 265:36-49. doi:10.1016/j.ins.2013.12.010
			Senia
			36
			49
			265","10.1016/j.ins.2013.12.010","0.154","-0.2669","3","Short texts, Lenguajes y sistemas informaticos","Short texts, Lenguajes y sistemas informaticos","",""
"dedup_wf_001::84c3f2bd2f86e2b77530e34fa9f94678","Statistics - Machine Learning","Squeezing bottlenecks: exploring the limits of autoencoder semantic representation capabilities","Gupta, Parth","2014","Elsevier","","","","","This is the author’s version of a work that was accepted for publication in Neurocomputing. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Neurocomputing 175 (2016) 1001–1008. DOI 10.1016/j.neucom.2015.06.091.
			We present a comprehensive study on the use of autoencoders for modelling text data, in which (differently
			from previous studies) we focus our attention on the various issues. We explore the suitability of
			two different models binary deep autencoders (bDA) and replicated-softmax deep autencoders (rsDA) for
			constructing deep autoencoders for text data at the sentence level. We propose and evaluate two novel
			metrics for better assessing the text-reconstruction capabilities of autoencoders. We propose an automatic
			method to find the critical bottleneck dimensionality for text representations (below which
			structural information is lost); and finally we conduct a comparative evaluation across different languages,
			exploring the regions of critical bottleneck dimensionality and its relationship to language perplexity.
			& 2015 Elsevier B.V. All rights reserved.
			A significant part of this research work was conducted during the first author's attachment to the HLT department of I2R in Singapore. The work of the first and third authors was carried out in the framework of the WIQ-EI IRSES project (Grant no. 269180) within the FP 7 Marie Curie, the DIANA APPLICATIONS Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) project and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Gupta, PA.; Banchs, R.; Rosso, P. (2016). Squeezing Bottlenecks: Exploring the Limits of Autoencoder Semantic Representation Capabilities. Neurocomputing. 175:1001-1008. doi:10.1016/j.neucom.2015.06.091
			Senia
			1001
			1008
			175","10.1016/j.neucom.2015.06.091","-0.0046","-0.3545","5","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","",""
"dedup_wf_001::8e5840f7868ddb59d1ceeb36758c2f1d","Arabic WordNet","On the evaluation and improvement of Arabic WordNet coverage and usability","Abouenour, Lahsen","2013","SPRINGER","","","LANGUAGE RESOURCES AND EVALUATION","","The final publication is available at Springer via http://dx.doi.org/10.1007/s10579-013-9237-0
			Built on the basis of the methods developed for Princeton WordNet and EuroWordNet, Arabic WordNet (AWN) has been an interesting project which combines WordNet structure compliance with Arabic particularities. In this paper, some AWN shortcomings related to coverage and usability are addressed. The use of AWN in question/answering (Q/A) helped us to deeply evaluate the resource from an experience-based perspective. Accordingly, an enrichment of AWN was built by semi-automatically extending its content. Indeed, existing approaches and/or resources developed for other languages were adapted and used for AWN. The experiments conducted in Arabic Q/A have shown an improvement of both AWN coverage as well as usability. Concerning coverage, a great amount of named entities extracted from YAGO were connected with corresponding AWN synsets. Also, a significant number of new verbs and nouns (including Broken Plural forms) were added. In terms of usability, thanks to the use of AWN, the performance for the AWN-based Q/A application registered an overall improvement with respect to the following three measures: accuracy (+9.27 % improvement), mean reciprocal rank (+3.6 improvement) and number of answered questions (+12.79 % improvement).
			The work presented in Sect. 2.2 was done in the framework of the bilateral Spain-Morocco AECID-PCI C/026728/09 research project. The research of the two first authors is done in the framework of the PROGRAMME D'URGENCE project (grant no. 03/2010). The research of the third author is done in the framework of WIQEI IRSES project (grant no. 269180) within the FP 7 Marie Curie People, DIANA-APPLICATIONS-Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) research project and VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems. We would like to thank Manuel Montes-y-Gomez (INAOE-Puebla, Mexico) and Sandra Garcia-Blasco (Bitsnbrain, Spain) for their feedback on the work presented in Sect. 2.4. We would like finally to thank Violetta Cavalli-Sforza (Al Akhawayn University in Ifrane, Morocco) for having reviewed the linguistic level of the entire document.
			Abouenour, L.; Bouzoubaa, K.; Rosso, P. (2013). On the evaluation and improvement of arabic wordnet coverage and usability. Language Resources and Evaluation. 47(3):891-917. doi:10.1007/s10579-013-9237-0
			Senia
			891
			917
			47
			3","10.1007/s10579-013-9237-0","0.1578","-0.6526","2","Arabic wordnet, Conditional random field","Arabic wordnet, Conditional random field","",""
"dedup_wf_001::953c54afc6a9037d0188885356b33a7b","Natural Language Processing","Identifying subjective statements in news titles using a personal sense annotation framework","Panicheva, Polina","2013","WILEY-BLACKWELL","","","JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE AND TECHNOLOGY","","This is the accepted version of the following article: Panicheva, P.; Cardiff, J.; Rosso, P. (2013). 
			Identifying subjective statements in news titles using a personal sense annotation framework. 
			Journal of the American Society for Information Science and Technology. 64(7):1411-1422
			, which has been published in final form at http://dx.doi.org/10.1002/asi.22841.
			Subjective language contains information about private states. The goal of subjective language identification is to determine that a private state is expressed, without considering its polarity or specific emotion. A component of word meaning, \"Personal Sense,\" has clear potential in the field of subjective language identification, as it reflects a meaning of words in terms of unique personal experience and carries personal characteristics. In this paper we investigate how Personal Sense can be harnessed for the purpose of identifying subjectivity in news titles. In the process, we develop a new Personal Sense annotation framework for annotating and classifying subjectivity, polarity, and emotion. The Personal Sense framework yields high performance in a fine-grained subsentence subjectivity classification. Our experiments demonstrate lexico-syntactic features to be useful for the identification of subjectivity indicators and the targets that receive the subjective Personal Sense.
			The work of Paolo Rosso was done within the EC WIQEI IRSES project (grant no. 269180) FP 7 Marie Curie People Framework, the MICINN Text-Enterprise 2.0 project (TIN2009-13391-C04-03) Plan I+D+I, and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems. We are grateful to the anonymous reviewers for helpful comments.
			Panicheva, P.; Cardiff, J.; Rosso, P. (2013). Identifying subjective statements in news titles using a personal sense annotation framework. Journal of the American Society for Information Science and Technology. 64(7):1411-1422. doi:10.1002/asi.22841
			Senia
			1411
			1422
			64
			7","10.1002/asi.22841","-0.6645","-0.0338","4","Sense annotation framework, Author profiling task, Authors gender","Sense annotation framework, Author profiling task, Authors gender","",""
"dedup_wf_001::a9f1358eacd7bd39531d162973f4bebf","Emotion detection","Exploring high-level features for detecting cyberpedophilia","Bogdanova, Dasha","2014","ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD","","","COMPUTER SPEECH AND LANGUAGE","","In this paper, we suggest a list of high-level features and study their applicability in detection of cyberpedophiles. We used a corpus of chats downloaded from http://www.perverted-justice.com and two negative datasets of different nature: cybersex logs available online, and the NPS chat corpus. The classification results show that the NPS data and the pedophiles’ conversations can be accurately discriminated from each other with character n-grams, while in the more complicated case of cybersex logs there is need for high-level features to reach good accuracy levels. In this latter setting our results show that features that model behaviour and emotion significantly outperform the low-level ones, and achieve a 97% accuracy.
			The work of Dasha Bogdanova was partially carried out during the internship at the Universitat Politecnica de Valencia (scholarship of the University of St. Petersburg). Her research was partially supported by Google Research Award. The collaboration with Thamar Solorio was possible thanks to her one-month research visit at the Universitat Politecnica de Valencia (program PAID-PAID-02-11 award n. 1932). The research work of Paolo Rosso was done in the framework of the European Commission WIQ-EI Web Information Quality Evaluation Initiative (IRSES Grant No. 269180) project within the FP 7 Marie Curie People, the DIANA-APPLICATIONS - Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-0O2-01) project, and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Bogdanova, D.; Rosso, P.; Solorio, T. (2014). Exploring high-level features for detecting cyberpedophilia. Computer Speech and Language. 28(1):108-120. doi:10.1016/j.csl.2013.04.007
			Senia
			108
			120
			28
			1","10.1016/j.csl.2013.04.007","-0.0409","-0.0196","7","Passage retrieval","Passage retrieval","",""
"dedup_wf_001::c2b11c60e1b2368e93e3a34d2d31a19c","integrated syntactic graphs","Automatic Authorship Detection Using Textual Patterns Extracted from Integrated Syntactic Graphs","Helena Gómez-Adorno","2016","MDPI AG","","","Sensors","","We apply the integrated syntactic graph feature extraction methodology to the task of automatic authorship detection. This graph-based representation allows integrating different levels of language description into a single structure. We extract textual patterns based on features obtained from shortest path walks over integrated syntactic graphs and apply them to determine the authors of documents. On average, our method outperforms the state of the art approaches and gives consistently high results across different corpora, unlike existing methods. Our results show that our textual patterns are useful for the task of authorship attribution.","10.3390/s16091374","0.658","0.1494","13","Features natural, Grams machine, Integrated syntactic graphs","Features natural, Grams machine, Integrated syntactic graphs","",""
"dedup_wf_001::c45a3da6db5d20deb0d9987aa4f06342","Passage retrieval","Voice-QA: evaluating the impact of misrecognized words on passage retrieval","Calvo Lance, Marcos","2012","Springer Verlag (Germany)","","","","https://hal.archives-ouvertes.fr/hal-00825246/document","Question Answering is an Information Retrieval task where the query is posed using natural language and the expected result is a concise answer. Voice-activated Question Answering systems represent an interesting application, where the question is formulated by speech. In these systems, an Automatic Speech Recognition module can be used to transcribe the question. Thus, recognition errors may be introduced, producing a significant effect on the answer retrieval process. In this work we study the relationship between some features of misrecognized words and the retrieval results. The features considered are the redundancy of a word in the result set and its inverse document frequency calculated over the collection. The results show that the redundancy of a word may be an important clue on whether an error on it would deteriorate the retrieval results, at least if a closed model is used for speech recognition.
			Text- Enterprise (TIN2009-13391-C04-03)
			Timpano (TIN2011-28169-C05-01)
			WIQEI IRSES (grant no. 269180) within the FP 7 Marie Curie People
			FPU Grant AP2010-4193 from the Spanish Ministerio de Educación
			Microcluster VLC/Campus on Multimodal Intelligent Systems
			Calvo Lance, M.; Buscaldi, D.; Rosso, P. (2012). Voice-QA: evaluating the impact of misrecognized words on passage retrieval. En Advances in Artificial Intelligence - IBERAMIA 2012. Springer Verlag (Germany). 462-471. doi:10.1007/978-3-642-34654-5_47
			Senia
			462
			471","10.1007/978-3-642-34654-5_47","0.4596","0.0684","7","Passage retrieval","Passage retrieval","",""
"dedup_wf_001::c5749a3da4f9e5c0e2cbb3742cf24746","Evaluation","Overview of the Evalita 2014 SENTIment POLarity Classification Task","Basile, Valerio","2014","HAL CCSD","","","","https://hal.inria.fr/hal-01228925/document","International audience; English. The SENTIment POLarity Classification Task (SENTIPOLC), a new shared task in the Evalita evaluation campaign , focused on sentiment classification at the message level on Italian tweets. It included three subtasks: subjectivity classification, polarity classification, and irony detection. SENTIPOLC was the most participated Evalita task with a total of 35 submitted runs from 11 different teams. We present the datasets and the evaluation methodology, and discuss results and participating systems. Italiano. Descriviamo modalit a e risultati della campagna di valutazione di sistemi di sentiment analysis (SENTIment POLarity Classification Task), proposta per la prima volta a \" Evalita–2014: Evaluation of NLP and Speech Tools for Ital-ian \". In SENTIPOLC e stata valutata la capacit a dei sistemi di riconoscere il sentiment espresso nei messaggi Twitter in lingua italiana. Sono stati proposti tre sotto-task: subjectivity classification, polarity classification e un sotto-task pilota di irony detection. La campagna ha susci-tato molto interesse e ricevuto un totale di 35 run inviati da 11 gruppi di partecipanti.","10.12871/clicit201429","-0.197","0.6865","11","Polarity classification task, Clef, Digital text forensics","Polarity classification task, Clef, Digital text forensics","",""
"dedup_wf_001::cacc1d6e63076e90b988a43b9de92e73","Irony detection","Making objective decisions from subjective data: Detecting irony in customer reviews","Reyes, Antonio","2012","ELSEVIER SCIENCE BV","","","DECISION SUPPORT SYSTEMS","","The research described in this work focuses on identifying key components for the task of irony detection. By means of analyzing a set of customer reviews, which are considered ironic both in social and mass media, we try to find hints about how to deal with this task from a computational point of view. Our objective is to gather a set of discriminating elements to represent irony, in particular, the kind of irony expressed in such reviews. To this end, we built a freely available data set with ironic reviews collected from Amazon. Such reviews were posted on the basis of an online viral effect; i.e. contents that trigger a chain reaction in people. The findings were assessed employing three classifiers. Initial results are largely positive, and provide valuable insights into the subjective issues of language facing tasks such as sentiment analysis, opinion mining and decision making. (C) 2012 Elsevier B.V. All rights reserved.
			National Council for Science and Technology (CONACyT - Mexico)
			European Commission [269180]
			MICINN Text-Enterprise [TIN2009-13391-C04-03]
			Reyes Pérez, A.; Rosso, P. (2012). Making objective decisions from subjective data: Detecting irony in customers reviews. Decision Support Systems. 53(4):754-760. doi:10.1016/j.dss.2012.05.027
			Senia
			754
			760
			53
			4","10.1016/j.dss.2012.05.027","-0.574","0.2313","1","Automatic irony detection, Detecting irony","Automatic irony detection, Detecting irony","",""
"dedup_wf_001::d0ee04b4fa5c1bddbfab206362e85b59","Apprentissage supervisé","L'étiquetage grammatical de l'amazighe en utilisant les propriétés n-grammes et un prétraitement de segmentation","Outahajala, Mohamed","2012","Ecole Mohammadia d’Ingénieurs","","","","","[FR] L’objectif de cet article est de présenter le premier étiqueteur grammatical amazighe. Très
			peu de ressources ont été développées pour l’amazighe et nous croyons que le
			développement d’un outil d’étiquetage grammatical est une étape préalable au traitement
			automatique de textes. Afin d'atteindre cet objectif, nous avons formé deux modèles de
			classification de séquences en utilisant les SVMs, séparateurs à vaste marge (Support Vector
			Machines) et les CRFs, champs markoviens conditionnels (Conditional Random Fields) en
			utilisant une phase de segmentation. Nous avons utilisé la technique de 10 fois la validation
			croisée pour évaluer notre approche. Les résultats montrent que les performances des SVMs
			et des CRFs sont très comparables. Dans l'ensemble, les SVMs ont légèrement dépassé les
			CRFs au niveau des échantillons (92,58% contre 92,14%) et la moyenne de précision des CRFs
			dépasse celle des SVMs (89,48% contre 89,29%). Ces résultats sont très prometteurs étant
			donné que nous avons utilisé un corpus de seulement ~ 20k mots.
			[EN] The aim of this paper is to present the first amazigh POS tagger. Very few linguistic resources
			have been developed so far for amazigh and we believe that the development of a POS tagger
			tool is the first step needed for automatic text processing. In order to achieve this endeavor,
			we have trained two sequence classification models using Support Vector Machines (SVMs)
			and Conditional Random Fields (CRFs) after using a tokenization step. We have used the 10-
			fold technique to evaluate our approach. Results show that the performance of SVMs and
			CRFs are very comparable. Across the board, SVMs outperformed CRFs on the fold level
			(92.58% vs. 92.14%) and CRFs outperformed SVMs on the 10 folds average level (89.48% vs.
			89.29%). These results are very promising considering that we have used a corpus of only ~20k
			tokens.
			EU FP7 Marie Curie PEOPLE-IRSES 269180 WiQ-Ei
			MICINN TEXT-ENTERPRISE 2.0 TIN2009-13391-C04-03 (Plan I+D+i)
			VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems
			Outahajala, M.; Benajiba, Y.; Rosso, P.; Zenkouar, L. (2012). L'étiquetage grammatical de l'amazighe en utilisant les propriétés n-grammes et un prétraitement de segmentation. E-TI : la revue électronique des technologies de l'information. 6:48-61. http://hdl.handle.net/10251/47570
			Senia
			48
			61
			6","","-0.4044","0.6076","6","Apprentissage supervisé","Apprentissage supervisé","",""
"dedup_wf_001::f7e5a04eb459d7fda530d6d4c8787c3e","Plagiarism detection systems","Plagiarism meets paraphrasing: insights for the new generation in automatic plagiarism detection","Barrón-Cedeño, Alberto","2013","The MIT Press","","","COMPUTATIONAL LINGUISTICS","","Although paraphrasing is the linguistic mechanism underlying many plagiarism cases, little attention has been paid to its analysis in the framework of automatic plagiarism detection. Therefore, state-of-the-art plagiarism detectors find it difficult to detect cases of paraphrase plagiarism. In this article, we analyze the relationship between paraphrasing and plagiarism, paying special attention to which paraphrase phenomena underlie acts of plagiarism and which of them are detected by plagiarism detection systems. With this aim in mind, we created the P4P corpus, a new resource that uses a paraphrase typology to annotate a subset of the PAN-PC-10 corpus for automatic plagiarism detection. The results of the Second International Competition on Plagiarism Detection were analyzed in the light of this annotation.The presented experiments show that (i) more complex paraphrase phenomena and a high density of paraphrase mechanisms make plagiarism detection more difficult, (ii) lexical substitutions are the paraphrase mechanisms used the most when plagiarizing, and (iii) paraphrase mechanisms tend to shorten the plagiarized text. For the first time, the paraphrase mechanisms behind plagiarism have been analyzed, providing critical insights for the improvement of automatic plagiarism detection systems.
			We would like to thank the people who participated in the annotation of the P4P corpus, Horacio Rodriguez for his helpful advice as experienced researcher, and the reviewers of this contribution for their valuable comments to improve this article. This research work was partially carried out during the tenure of an ERCIM \"Alain Bensoussan\" Fellowship Programme. The research leading to these results received funding from the EU FP7 Programme 2007-2013 (grant no. 246016), the MICINN projects TEXT-ENTERPRISE 2.0 and TEXT-KNOWLEDGE 2.0 (TIN2009-13391), the EC WIQ-EI IRSES project (grant no. 269180), and the FP7 Marie Curie People Programme. The research work of A. Barron-Cedeno and M. Vila was financed by the CONACyT-Mexico 192021 grant and the MECD-Spain FPU AP2008-02185 grant, respectively. The research work of A. Barron-Cedeno was partially done in the framework of his Ph.D. at the Universitat Politecnica de Valencia.
			Barrón Cedeño, LA.; Vila, M.; Martí, MA.; Rosso, P. (2013). Plagiarism meets paraphrasing: insights for the next generation in automatic plagiarism detection. Computational Linguistics. 39(4):917-947. doi:10.1162/COLI_a_00153
			Senia
			917
			947
			39
			4","10.1162/COLI_a_00153","-0.0935","0.4779","9","Plagiarism detection, Competition on plagiarism, International competition","Plagiarism detection, Competition on plagiarism, International competition","",""
"dedup_wf_001::fe39b30f30a3a4806d4026feebe5bd8d","Supervised classification","Determining and Characterizing the Reused Text for Plagiarism Detection","Sánchez-Vega, Fernando","2013","Elsevier","","","","","An important task in plagiarism detection is determining and measuring similar text portions between a
			given pair of documents. One of the main difficulties of this task resides on the fact that reused text is
			commonly modified with the aim of covering or camouflaging the plagiarism. Another difficulty is that
			not all similar text fragments are examples of plagiarism, since thematic coincidences also tend to produce
			portions of similar text. In order to tackle these problems, we propose a novel method for detecting
			likely portions of reused text. This method is able to detect common actions performed by plagiarists
			such as word deletion, insertion and transposition, allowing to obtain plausible portions of reused text.
			We also propose representing the identified reused text by means of a set of features that denote its
			degree of plagiarism, relevance and fragmentation. This new representation aims to facilitate the recognition
			of plagiarism by considering diverse characteristics of the reused text during the classification
			phase. Experimental results employing a supervised classification strategy showed that the proposed
			method is able to outperform traditionally used approaches.
			  2012 Elsevier Ltd. All rights reserved.
			This work was done under partial support of CONACyT project Grants: 134186, and Scholarships: 258345/224483. This work is the result of the collaboration in the framework of the WIQEI IRSES project (Grant No. 269180) within the FP 7 Marie Curie. The work of the last author was in the framework of the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Sánchez-Vega, F.; Villatoro-Tello, E.; Montes-Y-Gómez, M.; Villaseñor-Pineda; Luis; Rosso, P. (2013). Determining and Characterizing the Reused Text for Plagiarism Detection. Expert Systems with Applications. 40(5):1804-1813. doi:10.1016/j.eswa.2012.09.021
			Senia
			1804
			1813
			40
			5","10.1016/j.eswa.2012.09.021","0.0177","0.2086","3","Short texts, Lenguajes y sistemas informaticos","Short texts, Lenguajes y sistemas informaticos","",""
"narcis______::4f0e1a3cb3fb646cc9e4fdecdcedae5f","children deceptive;deceptive skills;effects children","Let’s lie together: Co-presence effects on children’s deceptive skills","Swerts, M.G.J.","2012","The Association for Computational Linguistics","","","","https://pure.uvt.nl/ws/files/1430213/Swertz_Proceeding_CADD_France_W12-04_let_s_lie_together__2_.pdf","","","0.3289","0.7516","15","Children deceptive, Deceptive skills, Effects children","Children deceptive, Deceptive skills, Effects children","",""
"od______1560::16777ca9b519612dbac205d863453b20","LENGUAJES Y SISTEMAS INFORMATICOS","Recent trends in digital text forensics and its evaluation","Gollub, Tim","2013","Springer Verlag (Germany)","","","","","The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-642-40802-1_28
			This paper outlines the concepts and achievements of our evaluation lab on digital text forensics, PAN 13, which called for original research and development on plagiarism detection, author identification, and author profiling. We present a standardized evaluation framework for each of the three tasks and discuss the evaluation results of the altogether 58 submitted contributions. For the first time, instead of accepting the output of software runs, we collected the softwares themselves and run them on a computer cluster at our site. As evaluation and experimentation platform we use TIRA, which is being developed at the Webis Group in Weimar. TIRA can handle large-scale software submissions by means of virtualization, sandboxed execution, tailored unit testing, and staged submission. In addition to the achieved evaluation results, a major achievement of our lab is that we now have the largest collection of state-of-the-art approaches with regard to the mentioned tasks for further analysis at our disposal.
			WIQ-EI IRSES project (Grant No. 269180) within the FP7 Marie Curie action
			Gollub, T.; Potthast, M.; Beyer, A.; Busse, M.; Rangel Pardo, FM.; Rosso, P.; Stamatatos, E.... (2013). Recent trends in digital text forensics and its evaluation. En Information Access Evaluation. Multilinguality, Multimodality, and Visualization. Springer Verlag (Germany). 282-302. doi:10.1007/978-3-642-40802-1_28
			Senia
			282
			302","10.1007/978-3-642-40802-1_28","0.2993","0.3191","11","Polarity classification task, Clef, Digital text forensics","Polarity classification task, Clef, Digital text forensics","",""
"od______1560::1f9fe8d0f185d3ca69c06da6bc9ef77c","Opinion spam","Detection of opinion spam with character n-grams","Hernández Fusilier, Donato","2015","Springer International Publishing","","","","","The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-18117-2_21
			In this paper we consider the detection of opinion spam as a stylistic classi cation task because, given a particular domain, the deceptive and truthful opinions are similar in content but di ffer in the way opinions are written (style). Particularly, we propose using character ngrams as features since they have shown to capture lexical content as well as stylistic information. We evaluated our approach on a standard
			corpus composed of 1600 hotel reviews, considering positive and negative
			reviews. We compared the results obtained with character n-grams against the ones with word n-grams. Moreover, we evaluated the e ffectiveness of character n-grams decreasing the training set size in order to simulate real training conditions. The results obtained show that character n-grams are good features for the detection of opinion spam; they seem to be able to capture better than word n-grams the content of deceptive opinions and the writing style of the deceiver. In particular,
			results show an improvement of 2:3% and 2:1% over the word-based representations in the detection of positive and negative deceptive opinions
			respectively. Furthermore, character n-grams allow to obtain a good performance
			also with a very small training corpus. Using only 25% of the training set, a Na  ve Bayes classi er showed F1 values up to 0.80 for both opinion polarities.
			WIQEI IRSES [269180] within the FP 7 Marie Curie
			LACCIR programme under project ID R1212LAC006
			DIANA-APPLICATIONS-Finding Hidden Knowledge in Texts: Applications [TIN2012-38603-C02-01]
			VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems
			Hernández Fusilier, D.; Montes Gomez, M.; Rosso, P.; Guzmán Cabrera, R. (2015). Detection of opinion spam with character n-grams. En Computational Linguistics and Intelligent Text Processing: 16th International Conference, CICLing 2015, Cairo, Egypt, April 14-20, 2015, Proceedings, Part II. Springer International Publishing. 285-294. doi:10.1007/978-3-319-18117-2_21
			Senia
			285
			294","10.1007/978-3-319-18117-2_21","-0.2824","0.0162","12","Opinion spam","Opinion spam","",""
"od______1560::20d4495712f430662ae4227eb61f0f1a","LENGUAJES Y SISTEMAS INFORMATICOS","On the Use of PU Learning for Quality Flaw Prediction in Wikipedia","Ferretti, Edgardo","2012","","","","","","In this article we describe a new approach to assess Quality Flaw Prediction in Wikipedia. The partially supervised method studied, called PU Learning, has been successfully applied in classi cations tasks with traditional corpora like Reuters-21578 or 20-Newsgroups. To the best of our knowledge, this is the  rst time that it is applied in this domain. Throughout this paper, we describe how the original PU Learning approach was evaluated for assessing quality flaws and the modi cations introduced to get a quality 
			aws predictor which obtained the best F1 scores in the task \Quality Flaw Prediction in Wikipedia\" of the PAN challenge.
			Universidad Nacional de San Luis [PROICO 30310]
			UNSL, INAOE and UPV by the European Commission as part of the WIQ-EI project
			CONACYT [134186]
			MICINN Text-Enterprise [TIN2009-13391-C04-03]
			Ferretti, E.; Hernández Fusilier, D.; Guzmán Cabrera, R.; Montes Y Gómez, M.; Errecalde, M.; Rosso, P. (2012). On the Use of PU Learning for Quality Flaw Prediction in Wikipedia. CEUR Workshop Proceedings. 1178. http://hdl.handle.net/10251/46566
			Senia
			1178","","0.5634","-0.4555","8","Lenguajes y sistemas informaticos","Lenguajes y sistemas informaticos","",""
"od______1560::27aadeea6be4c3854cd80fb60358c40b","Language translation and linguistics","Cross-language high similarity search using a conceptual thesaurus","Gupta, Parth","2012","Springer Verlag (Germany)","","","","","This work addresses the issue of cross-language high similarity and
			near-duplicates search, where, for the given document, a highly similar one is to
			be identified from a large cross-language collection of documents. We propose
			a concept-based similarity model for the problem which is very light in computation
			and memory. We evaluate the model on three corpora of different nature
			and two language pairs English-German and English-Spanish using the Eurovoc
			conceptual thesaurus. Our model is compared with two state-of-the-art models
			and we find, though the proposed model is very generic, it produces competitive
			results and is significantly stable and consistent across the corpora.
			Text-Enterprise 2.0  [TIN2009-13391-C04-03]
			CONACyT  [192021/302009]
			Gupta, P.; Barrón Cedeño, LA.; Rosso, P. (2012). Cross-language high similarity search using a conceptual thesaurus. En Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics. Springer Verlag (Germany). 7488:67-75. doi:10.1007/978-3-642-33247-0_8
			http://dx.doi.org/10.1007/978-3-642-33247-0_8
			Senia
			67
			75
			7488","10.1007/978-3-642-33247-0_8","0.6686","-0.1022","10","Approaches for measuring, Comparison of approaches, Conceptual thesaurus","Approaches for measuring, Comparison of approaches, Conceptual thesaurus","",""
"od______1560::3d0f10ed2242c3c03aabf468f8c3e9b6","Plagiarism detectors","Overview of the 4th International Competition on Plagiarism Detection","Potthast, Martin","2012","CLEF Initiative (Conference and Labs of the Evaluation Forum)","","","","","[EN] This paper overviews 15 plagiarism detectors that have been evaluated
			within the fourth international competition on plagiarism detection at PAN 12.
			We report on their performances for two sub-tasks of external plagiarism detection:
			candidate document retrieval and detailed document comparison. Furthermore,
			we introduce the PAN plagiarism corpus 2012, the TIRA experimentation
			platform, and the ChatNoir search engine for the ClueWeb. They add scale and
			realism to the evaluation as well as new means of measuring performance.
			EC WIQ-EI project (project no. 269180) within the FP7 People Program
			MICINN Text-Enterprise (TIN2009-13391-C04-03)
			ERCIM \"Alain Bensoussan\" Fellowship Programme (funded from the European Union Seventh Framework Programme FP7/2007-2013 under grant agreement number 246016)
			Potthast, M.; Gollub, T.; Hagen, M.; Graßegger, J.; Kiesel, J.; Michel, ML.; Oberländer, A.... (2012). Overview of the 4th International Competition on Plagiarism Detection. CLEF 2012 Evaluation Labs and Workshop – Working Notes Papers, 17-20 September. 101-128. http://hdl.handle.net/10251/55282
			Senia
			101
			128","","0.1028","0.5623","9","Plagiarism detection, Competition on plagiarism, International competition","Plagiarism detection, Competition on plagiarism, International competition","",""
"od______1560::4db21ecbe772fa81daaca1c037a37bc2","Automatic irony detection","Applying basic features from sentiment analysis on automatic irony detection","Hernández Farías, Irazú","2015","Springer International Publishing","","","","","The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-319-19390-8_38
			People use social media to express their opinions. Often linguistic devices such as irony are used. From the sentiment analysis perspective such utterances represent a challenge being a polarity reversor (usually from positive to negative). This paper presents an approach to address irony detection from a machine learning perspective. Our model considers structural features as well as, for the first time, sentiment analysis features such as the overall sentiment of a tweet and a score of its polarity. The approach has been evaluated over a set classifiers such as: Naïve Bayes, Decision Tree, Maximum Entropy, Support Vector Machine, and for the first time in irony detection task: Multilayer Perceptron. The results obtained showed the ability of our model to distinguish between potentially ironic and non-ironic sentences.
			The National Council for Science and Technology (CONACyT Mexico) [218109/313683, CVU-369616]
			WIQ-EI IRSES [269180] within the FP 7 Marie Curie
			DIANA-APPLI CATIONS [TIN2012-38603-C02-01]
			VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems
			Hernández Farías, I.; Benedí Ruiz, JM.; Rosso, P. (2015). Applying basic features from sentiment analysis on automatic irony detection. En Pattern Recognition and Image Analysis: 7th Iberian Conference, IbPRIA 2015, Santiago de Compostela, Spain, June 17-19, 2015, Proceedings. Springer International Publishing. 337-344. doi:10.1007/978-3-319-19390-8_38
			Senia
			337
			344","10.1007/978-3-319-19390-8_38","-0.3286","0.2976","1","Automatic irony detection, Detecting irony","Automatic irony detection, Detecting irony","",""
"od______1560::5e20534dc055002429d23dd0391b4a57","CLEF","Improving the Reproducibility of PAN s Shared Tasks","Potthast, Martin","2014","Springer Verlag (Germany)","","","","","This paper reports on the PAN 2014 evaluation lab which hosts three
			shared tasks on plagiarism detection, author identification, and author profiling.
			To improve the reproducibility of shared tasks in general, and PAN’s tasks in
			particular, the Webis group developed a new web service called TIRA, which
			facilitates software submissions. Unlike many other labs, PAN asks participants
			to submit running softwares instead of their run output. To deal with the
			organizational overhead involved in handling software submissions, the TIRA
			experimentation platform helps to significantly reduce the workload for both participants
			and organizers, whereas the submitted softwares are kept in a running
			state. This year, we addressed the matter of responsibility of successful execution
			of submitted softwares in order to put participants back in charge of executing
			their software at our site. In sum, 57 softwares have been submitted to our lab;
			together with the 58 software submissions of last year, this forms the largest collection
			of softwares for our three tasks to date, all of which are readily available
			for further analysis. The report concludes with a brief summary of each task.
			Potthast, M.; Gollub, T.; Rangel, F.; Rosso, P.; Stamatatos, E.; Stein, B. (2014). Improving the Reproducibility of PAN s Shared Tasks. En Information Access Evaluation. Multilinguality, Multimodality, and Interaction: 5th International Conference of the CLEF Initiative, CLEF 2014, Sheffield, UK, September 15-18, 2014. Proceedings. Springer Verlag (Germany). 268-299. doi:10.1007/978-3-319-11382-1_22
			Senia
			268
			299","10.1007/978-3-319-11382-1_22","0.4322","0.4906","11","Polarity classification task, Clef, Digital text forensics","Polarity classification task, Clef, Digital text forensics","",""
"od______1560::696e8bfc0610c1a21b326ddee3e6fb62","LENGUAJES Y SISTEMAS INFORMATICOS","Analysis of short texts on the Web: introduction to special issue","Rosso, Paolo","2013","Springer Netherlands","","","","","The final publication is available at Springer via http://dx.doi.org/10.1007/s10579-013-9220-9
			Analysis of web and social media data is a rapidly growing area of research. Researchers seek to extract a wide variety of information from these texts in order to address specific user needs, profile attitudes and intentions, and target advertising, etc., which may require application of the full range of natural processing techniques. However, many of the texts in question¿including news feeds, document titles, FAQs, and tweets¿exist as short, sometimes barely sentence-like snippets that do not always follow the lexical and syntactic conventions assumed by many language processing tools. Many NLP analyses rely on the repetition of specific lexical items throughout the text in order to identify topic, genre, and other features; without sufficient context to enable such analyses, and because of their often eccentric grammatical style, short texts pose a new kind of challenge for language processing research (Errecalde et al. 2008; Pinto et al. 2011).
			WIQ-EI IRSES project (Grant No. 269180) within the FP 7 Marie Curie People Framework of the European Commission
			Rosso, P.; Errecalde, ML.; Pinto Avendaño, D. (2013). Analysis of short texts on the Web: introduction to special issue. Language Resources and Evaluation. 47(1):123-126. doi:10.1007/s10579-013-9220-9
			Senia
			123
			126
			47
			1","10.1007/s10579-013-9220-9","0.0219","-0.5403","3","Short texts, Lenguajes y sistemas informaticos","Short texts, Lenguajes y sistemas informaticos","",""
"od______1560::7a8bb932fc98feaf603c1c76b89de19a","Short text analysis","Prototype/topic based Clustering Method for Weblogs","Perez-Tellez, Fernando","2016","IOS Press","","","","","In the last 10 years, the information generated on weblog sites has increased exponentially, resulting in a clear need for intelligent approaches to analyse and organise this massive amount of information. In this work, we present a methodology to cluster weblog posts according to the topics discussed therein, which we derive by text analysis. We have called the methodology Prototype/Topic Based Clustering, an approach which is based on a generative probabilistic model in conjunction with a Self-Term Expansion methodology. The usage of the Self-Term Expansion methodology is to improve the representation of the data and the generative probabilistic model is employed to identify relevant topics discussed in the weblogs. We have modified the generative probabilistic model in order to exploit predefined initialisations of the model and have performed our experiments in narrow and wide domain subsets. The results of our approach have demonstrated a considerable improvement over the pre-defined baseline and alternative state of the art approaches, achieving an improvement of up to 20% in many cases. The experiments were performed on both narrow and wide domain datasets, with the latter showing better improvement. However in both cases, our results outperformed the baseline and state of the art algorithms.
			The work of the third author was carried out in the framework of the WIQ-EI IRSES project (Grant No. 269180) within the FP7 Marie Curie, the DIANA APPLICATIONS Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) project and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Perez-Tellez, F.; Cardiff, J.; Rosso, P.; Pinto Avendaño, DE. (2016). Prototype/topic based Clustering Method for Weblogs. Intelligent Data Analysis. 20(1):47-65. doi:10.3233/IDA-150793
			Senia
			47
			65
			20
			1","10.3233/IDA-150793","0.2818","-0.5211","3","Short texts, Lenguajes y sistemas informaticos","Short texts, Lenguajes y sistemas informaticos","",""
"od______1560::a0dea5128954a86d9b6969eeefbfbc14","LENGUAJES Y SISTEMAS INFORMATICOS","The use of orthogonal similarity relations in the prediction of authorship","Sapkota, Upendra","2013","Springer Verlag (Germany)","","","","","The final publication is available at Springer via http://dx.doi.org/10.1007/978-3-642-37256-8_38
			Recent work on Authorship Attribution (AA) proposes the use of meta characteristics to train author models. The meta characteristics are orthogonal sets of similarity relations between the features from the different candidate authors. In that approach, the features are grouped and processed separately according to the type of information they encode, the so called linguistic modalities. For instance, the syntactic, stylistic and semantic features are each considered different modalities as they represent different aspects of the texts. The assumption is that the independent extraction of meta characteristics results in more informative feature vectors, that in turn result in higher accuracies. In this paper we set out to the task of studying the empirical value of this modality specific process. We experimented with different ways of generating the meta characteristics on different data sets with different numbers of authors and genres. Our results show that by extracting the meta characteristics from splitting features by their linguistic dimension we achieve consistent improvement of prediction accuracy.
			WIQ-EI project (project no. 269180) within the FP7 People Programme
			ONR grant N00014-12-1-0217
			NSF award 1254108
			CONACYT grant 134186
			Sapkota, U.; Solorio, T.; Montes Gómez, M.; Rosso, P. (2013). The use of orthogonal similarity relations in the prediction of authorship. En Computational Linguistics and Intelligent Text Processing. Springer Verlag (Germany). 463-475. doi:10.1007/978-3-642-37256-8_38
			Senia
			463
			475","10.1007/978-3-642-37256-8_38","0.3811","-0.147","7","Passage retrieval","Passage retrieval","",""
"od______1560::af0f05ef7ccbf1cf4d1f6affc49c8396","LENGUAJES Y SISTEMAS INFORMATICOS","Silhouette + Attraction: A Simple and Effective Method for Text Clustering","Errecalde, Marcelo","2015","Cambridge University Press (CUP): STM Journals","","","","","This article presents silhouette attraction (Sil Att), a simple and effective method for text clustering, which is based on two main concepts: the silhouette coefficient and the idea of attraction. The combination of both principles allows us to obtain a general technique that can be used either as a boosting method, which improves results of other clustering algorithms, or as an independent clustering algorithm. The experimental work shows that Sil Att is able to obtain high-quality results on text corpora with very different characteristics. Furthermore, its stable performance on all the considered corpora is indicative that it is a very robust method. This is a very interesting positive aspect of Sil Att with respect to the other algorithms used in the experiments, whose performances heavily depend on specific characteristics of the corpora being considered.
			This research work has been partially funded by UNSL, CONICET (Argentina), DIANA-APPLICATIONS-Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) research project, and the WIQ-EI IRSES project (grant no. 269180) within the FP 7 Marie Curie People Framework on Web Information Quality Evaluation Initiative. The work of the third author was done also in the framework of the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Errecalde, M.; Cagnina, L.; Rosso, P. (2015). Silhouette + Attraction: A Simple and Effective Method for Text Clustering. Natural Language Engineering. 1-40. doi:10.1017/S1351324915000273
			Senia
			1
			40","10.1017/S1351324915000273","0.295","-0.3811","3","Short texts, Lenguajes y sistemas informaticos","Short texts, Lenguajes y sistemas informaticos","",""
"od______1560::b1e0cce90435eda5d091e602a4a9f256","Affective processing","On the Impact of Emotions on Author Profiling","Rangel-Pardo, Francisco Manuel","2016","Elsevier","","","","","This is the author’s version of a work that was accepted for publication in Information Processing and Management. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Information Processing and Management 52 (2016) 73–92. DOI 10.1016/j.ipm.2015.06.003.
			In this paper, we investigate the impact of emotions on author profiling, concretely identifying
			age and gender. Firstly, we propose the EmoGraph method for modelling the way people use
			the language to express themselves on the basis of an emotion-labelled graph. We apply this
			representation model for identifying gender and age in the Spanish partition of the PAN-AP-13
			corpus, obtaining comparable results to the best performing systems of the PAN Lab of CLEF.
			© 2015 Elsevier B.V. All rights reserved.
			The work of the first author was partially funded by Autoritas Consulting SA and by Spanish Ministry of Economics under grant ECOPORTUNITY IPT-2012-1220-430000. The work of the second author was carried out in the framework of the WIQ-EI IRSES project (Grant No. 269180) within the FP 7 Marie Curie, the DIANA APPLICATIONS: Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) project and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems. A special mention to Maria Dolores Rangel Pardo for her linguistic contribution to this investigation.
			Rangel-Pardo, FM.; Rosso, P. (2016). On the Impact of Emotions on Author Profiling. Information Processing and Management. 52(1):73-92. doi:10.1016/j.ipm.2015.06.003
			Senia
			73
			92
			52
			1","10.1016/j.ipm.2015.06.003","-0.1444","-0.3399","5","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","",""
"od______1560::d44788662dee9d077c8b6050e0be534b","LENGUAJES Y SISTEMAS INFORMATICOS","Overview of the 3rd International Competition on Plagiarism Detection","Potthast, Martin","2011","CEUR Workshop Proceedings","","","","","This paper overviews eleven plagiarism detectors that have been developed
			and evaluated within PAN’11. We survey the detection approaches developed
			for the two sub-tasks “external plagiarism detection” and “intrinsic plagiarism
			detection,” and we report on their detailed evaluation based on the third
			revised edition of the PAN plagiarism corpus PAN-PC-11.
			MICINN as part of the Text-Enterprise 2.0 project [TIN2009-13391-C04-03]
			VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems
			Potthast, M.; Eiselt, A.; Barrón Cedeño, LA.; Stein, B.; Rosso, P. (2011). Overview of the 3rd International Competition on Plagiarism Detection. CEUR Workshop Proceedings. 1177. http://hdl.handle.net/10251/46639
			Senia
			1177","","0.01","0.5762","9","Plagiarism detection, Competition on plagiarism, International competition","Plagiarism detection, Competition on plagiarism, International competition","",""
"od______1560::dc9e814053e613a9a1529c68d5212a4c","Sentiment analysis","Emotion and Sentiment in Social and Expressive Media: Introduction to the special issue","Rosso, Paolo","2016","Elsevier","","","","","This is the author’s version of a work that was accepted for publication in Information Processing and Management . Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Information Processing and Management 52 (2016) 1–4. DOI  10.1016/j.ipm.2015.11.002
			Social and expressive media represent a challenge and a push forward for research on emotion and sentiment analysis. The
			advent of social media has brought about new paradigms of interaction that foster first-person engagement and crowdsourced
			contents: the subjective dimension moves to the foreground, opening the way to the emergence of an affective component
			within a dynamic corpus of digitized contents created and enriched by the users. Expressive media, which play a key role in
			fields related to creativity, such as figurative arts, music or drama, gather multimedia contents into online social environments,
			by joining the social dimension with the aims of artistic creation and self-expression. Artistic creation and performance seem
			to be a very interesting testbed for cross-validating and possibly integrating approaches, models and tools for automatically
			analyzing emotion and sentiment. In fact, in such contexts the social and affective dimensions (emotions and feelings) naturally
			emerge (Silvia, 2005), think for instance of the visitors’ feedback to a real or virtual art exhibition, or of the audience–performance
			interaction (...) In light of these considerations, this special issue focuses on the presentation and discussion of a set of novel computational
			approaches to the analysis of emotion and sentiment in social and expressive media.
			Rosso, P.; Bosco, C.; Damiano, R.; Patti, V.; Cambria, E. (2016). Emotion and Sentiment in Social and Expressive Media: Introduction to the special issue. Information Processing and Management. 52(1):1-4. doi:10.1016/j.ipm.2015.11.002
			Senia
			1
			4
			52
			1","10.1016/j.ipm.2015.11.002","-0.4474","-0.4345","5","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","Autoencoder semantic representation, Squeezing bottlenecks exploring, Emotion and sentiment","",""
"od______1560::e3dde2b43864e2942c0f9490bd6bded9","LENGUAJES Y SISTEMAS INFORMATICOS","Overview of the Author Profiling Task at PAN 2013","Rangel, Francisco","2013","CELCT","","","","","This overview presents the framework and results for the Author Profiling
			task at PAN 2013. We describe in detail the corpus and its characteristics,
			and the evaluation framework we used to measure the participants performance to
			solve the problem of identifying age and gender from anonymous texts. Finally,
			the approaches of the 21 participants and their results are described.
			Forensic Lab of the Universitat Pompeu Fabra Barcelona
			Autoritas Consulting SA and by Ministerio de Economía y Competitividad de España [ECOPORTUNITY IPT-2012-1220-430000]
			DIANA-APPLICATIONS [TIN2012-38603-C02-01]
			VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems
			Swiss National Science Foundation (SNF) project  [200021_130208]
			Rangel, F.; Rosso, P.; Koppel, M.; Stamatatos, E.; Inches, G. (2013). Overview of the Author Profiling Task at PAN 2013. CLEF Conference on Multilingual and Multimodal Information Access Evaluation. 352-365. http://hdl.handle.net/10251/46636
			Senia
			352
			365","","0.2381","0.1542","4","Sense annotation framework, Author profiling task, Authors gender","Sense annotation framework, Author profiling task, Authors gender","",""
"od______1560::ef1e116e99a951614ad9a9551396dfb5","Affective processing","On the Identification of Emotions and Authors' Gender in Facebook Comments on the Basis of their Writing Style","Rangel, Francisco","2013","CEUR Workshop Proceedings","","","","","In this paper, we propose a method for automatic identifying
			emotions in written texts in social media with high proliferation such as
			Facebook. For that task we try to model the way people use the language
			to express themselves, and also use this model for identifying the gender
			of the authors. We focused on Spanish due to the lack of studies and
			resources in that language.
			Autoritas Consulting SA
			Ministerio de Economía de España [ECOPORTUNITY IPT-2012-1220-430000]
			FP 7 Marie Curie [WIQ-EI IRSES]
			VLC/CAMPUS [TIN2012-38603-C02-01]
			Rangel, F.; Rosso, P. (2013). On the Identification of Emotions and Authors' Gender in Facebook Comments on the Basis of their Writing Style. CEUR Workshop Proceedings. 1096:34-46. http://hdl.handle.net/10251/38110
			Senia
			34
			46
			1096","","-0.2872","-0.5386","4","Sense annotation framework, Author profiling task, Authors gender","Sense annotation framework, Author profiling task, Authors gender","",""
"od______1560::f8863ccdff716eab789d1d873e0178e6","Opinion mining","Detecting Positive and Negative Deceptive Opinions using PU-learning","HERNÁNDEZ FUSILIER, DONATO","2015","Elsevier","","","","","Nowadays a large number of opinion reviews are posted on the Web. Such reviews are a
			very important source of information for customers and companies. The former rely more
			than ever on online reviews to make their purchase decisions, and the latter to respond
			promptly to their clients’ expectations. Unfortunately, due to the business that is behind,
			there is an increasing number of deceptive opinions, that is, fictitious opinions that have
			been deliberately written to sound authentic, in order to deceive the consumers
			promoting a low quality product (positive deceptive opinions) or criticizing a potentially
			good quality one (negative deceptive opinions). In this paper we focus on the detection of
			both types of deceptive opinions, positive and negative. Due to the scarcity of examples of
			deceptive opinions, we propose to approach the problem of the detection of deceptive
			opinions employing PU-learning. PU-learning is a semi-supervised technique for building
			a binary classifier on the basis of positive (i.e., deceptive opinions) and unlabeled
			examples only. Concretely, we propose a novel method that with respect to its original
			version is much more conservative at the moment of selecting the negative examples
			(i.e., not deceptive opinions) from the unlabeled ones. The obtained results show that
			the proposed PU-learning method consistently outperformed the original PU-learning
			approach. In particular, results show an average improvement of 8.2% and 1.6% over the
			original approach in the detection of positive and negative deceptive opinions
			respectively.
			 2014 Elsevier Ltd. All rights reserved.
			This work is the result of the collaboration in the framework of the WIQEI IRSES project (Grant No. 269180) within the FP 7 Marie Curie. The work of the third author was in the framework the DIANA-APPLICATIONS-Finding Hidden Knowledge in Texts: Applications (TIN2012-38603-C02-01) project, and the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems.
			Hernández Fusilier, D.; Montes Gómez, M.; Rosso, P.; Guzmán Cabrera, R. (2015). Detecting Positive and Negative Deceptive Opinions using PU-learning. Information Processing and Management. 51(4):433-443. doi:10.1016/j.ipm.2014.11.001
			Senia
			433
			443
			51
			4","10.1016/j.ipm.2014.11.001","-0.4376","-0.0757","12","Opinion spam","Opinion spam","",""
"od______2659::1ff05a594483ea93d59007fb811a1e2a","content facets;media","
						Content Facets for Individual Information Needs in Media
					","","","Zenodo","","","","","
						<p>The amount of content published in traditional media is huge and steadily growing. Additionally, social media gained momentum since people use blogs to share comments and opinions to current events. However, blog content is questionable in respect to quality since blogs are neither reviewed nor edited. From the media consumer perspective, navigating the haystack of information produced by media as well as finding content that meets ones quality demands is challenging. This challenge is the key motivation for this thesis: to support media consumers to filter media content by content facets capturing topical information needs as well as content quality aspects.</p>

<p>For this, two types of content facets are suggested: (i) topic oriented and (ii) topic independent quality related content facets. First, for each facet type (i) and (ii), concrete content facets are proposed. These content facets are then formally defined. Second, a feature study revealed that stylometric features are better suited to assess topic independent content facets, while for topic oriented content facets Bag-of-Words features serve best. Third, the best features have successfully been used to classify traditional and social media content in both types of content facets. To address the problem of lacking training data in classification, this thesis investi- gated whether available classification schemes from traditional media can be mapped onto blogs. The experiments revealed that traditional media correlate content wise with selected blogs; therefore, content facets from traditional media can be applied to blogs. Several proposed content facets have successfully been implemented in APA Labs, a Web-based framework for faceted search in traditional and social me- dia. Consequently, APA Labs supports media consumers to navigate and analyze traditional and social media content. This enables any media consumer to search for information according to their personal information need. This is a substantial improvement to individualize search in media.&nbsp;</p>
					","10.5281/zenodo.1195993","-0.6608","-0.3586","14","Content facets, Media","Content facets, Media","",""
"od______2659::728ad8bc36689ab41527254d5bc3b4c0","content facets;media","
						Content Facets for Individual Information Needs in Media
					","","","Zenodo","","","","","
						<p>The amount of content published in traditional media is huge and steadily growing. Additionally, social media gained momentum since people use blogs to share comments and opinions to current events. However, blog content is questionable in respect to quality since blogs are neither reviewed nor edited. From the media consumer perspective, navigating the haystack of information produced by media as well as finding content that meets ones quality demands is challenging. This challenge is the key motivation for this thesis: to support media consumers to filter media content by content facets capturing topical information needs as well as content quality aspects.</p>

<p>For this, two types of content facets are suggested: (i) topic oriented and (ii) topic independent quality related content facets. First, for each facet type (i) and (ii), concrete content facets are proposed. These content facets are then formally defined. Second, a feature study revealed that stylometric features are better suited to assess topic independent content facets, while for topic oriented content facets Bag-of-Words features serve best. Third, the best features have successfully been used to classify traditional and social media content in both types of content facets. To address the problem of lacking training data in classification, this thesis investi- gated whether available classification schemes from traditional media can be mapped onto blogs. The experiments revealed that traditional media correlate content wise with selected blogs; therefore, content facets from traditional media can be applied to blogs. Several proposed content facets have successfully been implemented in APA Labs, a Web-based framework for faceted search in traditional and social me- dia. Consequently, APA Labs supports media consumers to navigate and analyze traditional and social media content. This enables any media consumer to search for information according to their personal information need. This is a substantial improvement to individualize search in media.&nbsp;</p>
					","10.5281/zenodo.1196397","-0.6608","-0.3586","14","Content facets, Media","Content facets, Media","",""
"webcrawl____::3d6f3bd03ff6652a69cf05f00a06dbac","features natural;grams machine;language processing","Syntactic N-grams as machine learning features for natural language processing","Sidorov, Grigori","2014","PERGAMON-ELSEVIER SCIENCE LTD","","","EXPERT SYSTEMS WITH APPLICATIONS","","","10.1016/j.eswa.2013.08.015","0.669","0.3135","13","Features natural, Grams machine, Integrated syntactic graphs","Features natural, Grams machine, Integrated syntactic graphs","",""
"webcrawl____::3d7113e3265396764fee1322dc757420","approach detecting;detecting irony;irony twitter","A multidimensional approach for detecting irony in Twitter","Reyes, Antonio","2013","SPRINGER","","","LANGUAGE RESOURCES AND EVALUATION","","","10.1007/s10579-012-9196-x","-0.5836","0.4316","1","Automatic irony detection, Detecting irony","Automatic irony detection, Detecting irony","",""
