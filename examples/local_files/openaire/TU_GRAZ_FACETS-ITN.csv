"doi","id","subject","title","authors","year","publisher","resulttype","language","journal","url","paper_abstract","project_id","accessright","x","y","area_uri","cluster_labels","area","cited_by_tweeters_count","readers.mendeley","citation_count","readers","file_hash"
"","dedup_wf_001::0ed89e46b6dd53836e69a3f44b4013a8","FPGA","SYSTEMES NEUROMORPHIQUES POUR DES PLATEFORMES BIO-HYBRIDES","Levi, Timothée","2017-03-02","HAL CCSD","publication","","","https://hal.archives-ouvertes.fr/tel-01540207v3/document","Le présent mémoire d’habilitation à diriger des recherches propose une synthèse de mes activités d’enseignement et de recherche sur la période 2004-2016. le contexte scientifique général de ce mémoire est celui des Systèmes neuromorphiques pour des plateformes bio-hybrides. Un système biomimétique (neuromimétique) se distingue d’un système bio-inspiré (neuro-inspiré) par son objectif et ses caractéristiques. En effet, un système bio-inspiré est un système s’inspirant de la nature pour effectuer des tâches d’ingénierie comme la reconnaissance de forme par exemple. Il ne fonctionne pas forcément en temps réel biologique mais plutôt en temps accéléré. La plupart de ses applications concernent des tâches de calculs ou la robotique. Un système biomimétique reproduit le comportement de la nature en en étant le plus proche possible. Ces systèmes travaillent en temps réel biologique et peuvent ainsi remplacer le vivant. Leurs applications sont plus tournées vers le biomédical, vers la compréhension des systèmes vivants et pour la création de neuroprothèses en ce qui concerne les systèmes neuromimétiques. Les systèmes neuromimétiques que nous avons conçu dans notre équipe seront développés dans le deuxième chapitre. Ces systèmes étant également hétérogènes (électronique analogique, numérique, optique, chimique et microfluidique), une méthodologie de modélisation pour l’électronique analogique et mixte est obligatoire et sera développée dans le premier chapitre.Mes travaux de recherche ont été effectué dans l’objectif général de concevoir des systèmes neuromimétiques novateurs afin d’effectuer des expériences hybrides avec le vivant pour la compréhension des différents mécanismes mais aussi pour permettre de remplacer le vivant pour certaines applications. Cela nécessite avant tout de proposer plusieurs systèmes utilisant différents modèles et diverses plateformes.Les 3 premiers chapitres de ce mémoire s’inscrivent pleinement dans cet objectif et déclinent ma contribution selon les 3 axes suivants :- Le chapitre 1 est consacré aux méthodes de modélisation et de réutilisation permettant d’optimiser la conception et le fonctionnement de ces systèmes. Ces systèmes étant hétérogènes, développer des méthodologies communes permet d’augmenter l’efficacité et la rapidité de conception.- Le chapitre 2 fait l’exposé des différents systèmes neuromimétiques conçus lors de mes activités de recherche. Allant de l’électronique analogique à la microfluidique en passant par de l’électronique numérique, ces réseaux de neurones artificiels gardent le même objectif de l’hybridation avec le vivant.- Le chapitre 3 décrit mes projets de recherche que ce soit à court-terme ou à long-terme. Deux axes se distinguent : la conception de plateforme neuromorphique bio-hybride et les systèmes neuromorphiques au profit de l’intelligence artificielle.- Le chapitre 4 s’attache à mes activités de collaboration et de gestion des relations internationales que j’effectue au cours de mes activités de recherche mais également dans le cadre de mes enseignements à l’IUT GEII de Bordeaux.","237955","Open Access","0.8207","-0.0137","9","Fpga","Fpga",NA,NA,NA,"",""
"","dedup_wf_001::b34be168b3a885360dec7703679b0236","Réseaux neuronaux stochastiques","Traiter le cerveau avec les neurosciences : théorie de champ-moyen, effets de taille finie et capacité de codage des réseaux de neurones stochastiques","Fasoli, Diego","2013-09-25","HAL CCSD","publication","","","https://tel.archives-ouvertes.fr/tel-00850289v2/document","The brain is the most complex system in the known universe. Its nested structure with small-world properties determines its function and behavior. The analysis of its structure requires sophisticated mathematical and statistical techniques. In this thesis we shed new light on neural networks, attacking the problem from different points of view, in the spirit of the Theory of Complexity and in terms of their information processing capabilities. In particular, we quantify the Fisher information of the system, which is a measure of its encoding capability. The first technique developed in this work is the mean-field theory of rate and FitzHugh-Nagumo networks without correlations in the thermodynamic limit, through both mathematical and numerical analysis. The second technique, the Mayer’s cluster expansion, is taken from the physics of plasma, and allows us to determine numerically the finite size effects of rate neurons, as well as the relationship of the Fisher information to the size of the network for independent Brownian motions. The third technique is a perturbative expansion, which allows us to determine the correlation structure of the rate network for a variety of different types of connectivity matrices and for different values of the correlation between the sources of randomness in the system. With this method we can also quantify numerically the Fisher information not only as a function of the network size, but also for different correlation structures of the system. The fourth technique is a slightly different type of perturbative expansion, with which we can study the behavior of completely generic connectivity matrices with random topologies. Moreover this method provides an analytic formula for the Fisher information, which is in qualitative agreement with the other results in this thesis. Finally, the fifth technique is purely numerical, and uses an Expectation-Maximization algorithm and Monte Carlo integration in order to evaluate the Fisher information of the FitzHugh-Nagumo network. In summary, this thesis provides an analysis of the dynamics and the correlation structure of the neural networks, confirms this through numerical simulation and makes two key counterintuitive predictions. The first is the formation of a perfect correlation between the neurons for particular values of the parameters of the system, a phenomenon that we term stochastic synchronization. The second, which is somewhat contrary to received opinion, is the explosion of the Fisher information and therefore of the encoding capability of the network for highly correlated neurons. The techniques developed in this thesis can be used also for a complete quantification of the information processing capabilities of the network in terms of information storage, transmission and modification, but this would need to be performed in the future.; Ce travail a été développé dans le cadre du projet européen FACETS-ITN, dans le domaine des Neurosciences Computationnelles. Son but est d’améliorer la compréhension des réseaux de neurones stochastiques de taille finie, pour des sources corrélées à caractère aléatoire et pour des matrices de connectivité biologiquement réalistes. Ce résultat est obtenu par l’analyse de la matrice de corrélation du réseau et la quantification de la capacité de codage du système en termes de son information de Fisher. Les méthodes comprennent diverses techniques mathématiques, statistiques et numériques, dont certaines ont été importés d’autres domaines scientifiques, comme la physique et la théorie de l’estimation. Ce travail étend de précédents résultats fondées sur des hypothèses simplifiées qui ne sont pas réaliste d’un point de vue biologique et qui peuvent être pertinents pour la compréhension des principes de travail liés cerveau. De plus, ce travail fournit les outils nécessaires à une analyse complète de la capacité de traitement de l’information des réseaux de neurones, qui sont toujours manquante dans la communauté scientifique.","227747","Open Access","0.0134","-0.5836","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks",NA,NA,NA,"",""
"","od______3364::a9801e9107ed8d1c390546cd5012b4c2","balanced networks;correlations balanced;invariance correlations","Structure and invariance of correlations in balanced networks.","Helias, Moritz","2013-01-01","","publication","","","","","269921","Closed Access","-0.223","-0.7495","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks",NA,NA,NA,"",""
"","od________18::b4454f3ebe1435875c3422db880dff72","Quantitative Biology - Neurons and Cognition","Untersuchungen zur Modellierung und Schaltungsrealisierung von synaptischer Plastizitaet","Mayr, Christian","2014-09-06","","publication","","","","This manuscript deals with the analysis and VLSI implementation of adaptive information processing derived from biological measurements. Specifically, models for short term plasticity, long term plasticity and metaplasticity are derived from biological measurements and implemented in CMOS circuits.","269921","Open Access","-0.4962","0.5233","7","Independent component analysis, Conditions for spiking, General conditions","Independent component analysis, Conditions for spiking, General conditions",NA,NA,NA,"",""
"","dedup_wf_001::f1d8f518ba782b14a321ba789b4dc403","Hodgkin and Huxley model","Réseau de neurones in silico : contribution au développement de la technique hybride pour les réseaux corticaux","Grassia, Filippo Giovanni","2013-01-07","HAL CCSD","publication","","","https://tel.archives-ouvertes.fr/tel-00789406/document","This work has been supported by the European FACETS-ITN project. Within the frameworkof this project, we contribute to the simulation of cortical cell types (employingexperimental electrophysiological data of these cells as references), using a specific VLSIneural circuit to simulate, at the single cell level, the models studied as references in theFACETS project. The real-time intrinsic properties of the neuromorphic circuits, whichprecisely compute neuron conductance-based models, will allow a systematic and detailedexploration of the models, while the physical and analog aspect of the simulations, as opposedthe software simulation aspect, will provide inputs for the development of the neuralhardware at the network level. The second goal of this thesis is to contribute to the designof a mixed hardware-software platform (PAX), specifically designed to simulate spikingneural networks. The tasks performed during this thesis project included: 1) the methodsused to obtain the appropriate parameter sets of the cortical neuron models that can beimplemented in our analog neuromimetic chip (the parameter extraction steps was validatedusing a bifurcation analysis that shows that the simplified HH model implementedin our silicon neuron shares the dynamics of the HH model); 2) the fully customizablefitting method, in voltage-clamp mode, to tune our neuromimetic integrated circuits usinga metaheuristic algorithm; 3) the contribution to the development of the PAX systemin terms of software tools and a VHDL driver interface for neuron configuration in theplatform. Finally, it also addresses the issue of synaptic tuning for future SNN simulation.; Ces travaux ont été menés dans le cadre du projet européen FACETS-ITN. Nous avons contribué à la simulation de cellules corticales grâce à des données expérimentales d'électrophysiologie comme référence et d'un circuit intégré neuromorphique comme simulateur. Les propriétés intrinsèques temps réel de nos circuits neuromorphiques à base de modèles à conductance, autorisent une exploration détaillée des différents types de neurones. L'aspect analogique des circuits intégrés permet le développement d'un simulateur matériel temps réel à l'échelle du réseau. Le deuxième objectif de cette thèse est donc de contribuer au développement d'une plate-forme mixte - matérielle et logicielle - dédiée à la simulation de réseaux de neurones impulsionnels.","237955","Open Access","0.4257","0.0721","2","Bifurcation analysis, Contribution au développement, Cortical neuron models","Bifurcation analysis, Contribution au développement, Cortical neuron models",NA,NA,NA,"",""
"","od________18::d77fd5fd94f2f4b8a1fbf277f8cae04b","Quantitative Biology - Neurons and Cognition","Stochastic inference with deterministic spiking neurons","Petrovici, Mihai A.","2013-11-13","","publication","","","","The seemingly stochastic transient dynamics of neocortical circuits observed in vivo have been hypothesized to represent a signature of ongoing stochastic inference. In vitro neurons, on the other hand, exhibit a highly deterministic response to various types of stimulation. We show that an ensemble of deterministic leaky integrate-and-fire neurons embedded in a spiking noisy environment can attain the correct firing statistics in order to sample from a well-defined target distribution. We provide an analytical derivation of the activation function on the single cell level; for recurrent networks, we examine convergence towards stationarity in computer simulations and demonstrate sample-based Bayesian inference in a mixed graphical model. This establishes a rigorous link between deterministic neuron models and functional stochastic dynamics on the network level.","269921","Open Access","0.0068","-0.1283","13","Spiking neurons","Spiking neurons",NA,NA,NA,"",""
"10.1007/s00422-011-0435-9","dedup_wf_001::1a6c2c01c6f3399aeeda2c5a15064ad8","Quantitative Biology - Neurons and Cognition","A comprehensive workflow for general-purpose neural modeling with highly configurable neuromorphic hardware systems","Bruederle, Daniel","2011-01-01","SPRINGER","publication","","BIOLOGICAL CYBERNETICS","","In this paper we present a methodological framework that meets novel requirements emerging from upcoming types of accelerated and highly configurable neuromorphic hardware systems. We describe in detail a device with 45 million programmable and dynamic synapses that is currently under development, and we sketch the conceptual challenges that arise from taking this platform into operation. More specifically, we aim at the establishment of this neuromorphic system as a flexible and neuroscientifically valuable modeling tool that can be used by non-hardware-experts. We consider various functional aspects to be crucial for this purpose, and we introduce a consistent workflow with detailed descriptions of all involved modules that implement the suggested steps: The integration of the hardware interface into the simulator-independent model description language PyNN; a fully automated translation between the PyNN domain and appropriate hardware configurations; an executable specification of the future neuromorphic system that can be seamlessly integrated into this biology-to-hardware mapping process as a test bench for all software layers and possible hardware design modifications; an evaluation scheme that deploys models from a dedicated benchmark library, compares the results generated by virtual or prototype hardware devices with reference software simulations and analyzes the differences. The integration of these components into one hardware-software workflow provides an ecosystem for ongoing preparative studies that support the hardware design process and represents the basis for the maturity of the model-to-hardware mapping software. The functionality and flexibility of the latter is proven with a variety of experimental results.","237955","Open Access","0.5921","0.2553","3","General purpose, Neuromorphic hardware, Synaptic weight resolution","General purpose, Neuromorphic hardware, Synaptic weight resolution","1","117","40","",""
"10.1007/s10015-012-0016-6","dedup_wf_001::ff13f737cd908e701a6d58086f1a6b10","Quantitative Biology::Neurons and Cognition","Bifurcation analysis in a silicon neuron","Grassia, Filippo","2012-10-01","HAL CCSD","publication","","","https://hal.archives-ouvertes.fr/hal-00766340/document","International audience; In this paper, we describe an analysis of the nonlinear dynamical phenomenon associated with a silicon neuron. Our silicon neuron integrates Hodgkin-Huxley (HH) model formalism, including the membrane voltage dependency of temporal dynamics. Analysis of the bifurcation conditions allow us to identify different regimes in the parameter space that are desirable for biasing our silicon neuron. This approach of studying bifurcations is useful because it is believed that computational properties of neurons are based on the bifurcations exhibited by these dynamical systems in response to some changing stimulus. We describe numerical simulations and measurements of the Hopf bifurcation which is characteristic of class 2 excitability in the HH model. We also show a phenomenon observed in biological neurons and termed excitation block. Hence, by showing that this silicon neuron has similar bifurcations to a certain class of biological neurons, we can claim that the silicon neuron can also perform similar computations","237955","Open Access","-0.3799","-0.0374","2","Bifurcation analysis, Contribution au développement, Cortical neuron models","Bifurcation analysis, Contribution au développement, Cortical neuron models",NA,NA,"2","",""
"10.1007/s10827-012-0413-9","dedup_wf_001::33aa54e1d4f9c7332f2a76752a80fbf5","Recurrent network dynamics","High-capacity embedding of synfire chains in a cortical network model","Trengove, Chris","2012-08-01","Springer US","publication","","Journal of Computational Neuroscience","","Synfire chains, sequences of pools linked by feedforward connections, support the propagation of precisely timed spike sequences, or synfire waves. An important question remains, how synfire chains can efficiently be embedded in cortical architecture. We present a model of synfire chain embedding in a cortical scale recurrent network using conductance-based synapses, balanced chains, and variable transmission delays. The network attains substantially higher embedding capacities than previous spiking neuron models and allows all its connections to be used for embedding. The number of waves in the model is regulated by recurrent background noise. We computationally explore the embedding capacity limit, and use a mean field analysis to describe the equilibrium state. Simulations confirm the mean field analysis over broad ranges of pool sizes and connectivity levels; the number of pools embedded in the system trades off against the firing rate and the number of waves. An optimal inhibition level balances the conflicting requirements of stable synfire propagation and limited response to background noise. A simplified analysis shows that the present conductance-based synapses achieve higher contrast between the responses to synfire input and background noise compared to current-based synapses, while regulation of wave numbers is traced to the use of variable transmission delays.","237955","Open Access","0.1663","0.2878","8","Network model, Spiking neural network","Network model, Spiking neural network",NA,NA,"9","",""
"10.1016/j.jphysparis.2013.08.001","dedup_wf_001::c86ddc0258eead0e3ae1c04c4c4c4031","motion based;based prediction;explains role","Motion-based prediction explains the role of tracking in motion extrapolation","Khoei, Mina A.","2013-01-01","ELSEVIER SCI LTD","publication","","JOURNAL OF PHYSIOLOGY-PARIS","","","237955","Closed Access","0.6083","-0.5235","1","Motion based prediction","Motion based prediction",NA,NA,"7","",""
"10.1038/ncomms7922","dedup_wf_001::d589b55192d96952a7c079227c567cb1","Article","Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks","Zenke, Friedemann","2015-04-01","Nature Publishing Group","publication","","Nature Communications","","Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.","269921","Open Access","-0.3413","0.3343","15","Synaptic plasticity, Neural networks","Synaptic plasticity, Neural networks","18","306","50","",""
"10.1038/srep26029","dedup_wf_001::20cfc7ece906af00fe91743992271f24","Neuronal networks","Dynamical state of the network determines the efficacy of single neuron properties in shaping the network activity","Sahasranamam, Ajith","2016-01-01","Bernstein Center Freiburg, University of Freiburg, Freiburg, Germany","publication","","","","Spike patterns are among the most common electrophysiological descriptors of neuron types. Surprisingly, it is not clear how the diversity in firing patterns of the neurons in a network affects its activity dynamics. Here, we introduce the state-dependent stochastic bursting neuron model allowing for a change in its firing patterns independent of changes in its input-output firing rate relationship. Using this model, we show that the effect of single neuron spiking on the network dynamics is contingent on the network activity state. While spike bursting can both generate and disrupt oscillations, these patterns are ineffective in large regions of the network state space in changing the network activity qualitatively. Finally, we show that when single-neuron properties are made dependent on the population activity, a hysteresis like dynamics emerges. This novel phenomenon has important implications for determining the network response to time-varying inputs and for the network sensitivity at different operating points. 
			<p>QC 20160525</p>","237955","Open Access","-0.2626","-0.1812","6","Single neuron","Single neuron","3","48","3","",""
"10.1051/jp4:19945302","dedup_wf_001::d0d57a13103590a82b224727226f1a35","Membrane potential","Dynamique stochastique non-stationnaire de la membrane neuronale","Ferreira Brigham, Marco Paulo","2015-04-27","HAL CCSD","publication","","","https://tel.archives-ouvertes.fr/tel-01186526/document","Neurons interact through their membrane potential that generally has a complex time evolution due to numerous irregular synaptic inputs received. This complex time evolution is best described in probabilistic terms due to this irregular or \"noisy\" activity. The time evolution of the membrane potential is therefore both stochastic and deterministic: it is stochastic since it is driven by random input arrival times, but also deterministic, since subjecting a biological neuron to the same sequence of input arrival times often results in very similar membrane potential traces. In this thesis, we investigated key statistical properties of a simplified neuron model under nonstationary input from other neurons that results in nonstationary evolution of membrane potential statistics. We considered a passive neuron model without spiking mechanism that is driven by input currents or conductances in the form of shot noise processes. Under such input, membrane potential fluctuations can be modeled as filtered shot noise currents or conductances. We analyzed the statistical properties of these filtered processes in the framework of Poisson Point Processes transformations. The key idea is to express filtered shot noise as a transformation of random input arrival times and to apply the properties of these transformations to derive its nonstationary statistics. Using this formalism we derive exact analytical expressions, and useful approximations, for the mean and joint cumulants of the filtered process in the general case of variable input rate. This work opens many perspectives for analyzing neurons under in vivo conditions, in the presence of intense and noisy synaptic inputs.; Les neurones interagissent à travers leur potentiel de membrane qui a en général une évolution temporelle complexe due aux nombreuses entrées synaptiques irrégulières reçues. Cette évolution est mieux décrite en termes probabilistes, en raison de ces entrées irrégulières ou «bruit synaptique». L'évolution temporelle du potentiel de membrane est stochastique mais aussi déterministe: stochastique, car conduite par des entrées synaptiques qui arrivent de façon aléatoire dans le temps, et déterministe, car un neurone biologique a une évolution temporelle très similaire quand soumis à une même séquence d'entrées synaptiques. Nous étudions les propriétés statistiques d'un modèle simplifié de neurone soumis à des entrées à taux variable d'où en résulte l'évolution non-stationnaire du potentiel de membrane. Nous considérons un modèle passif de membrane neuronale, sans mécanisme de décharge neuronale, soumis à des entrées à courant ou à conductance sous la forme d'un processus de «shot noise». Les fluctuations du potentiel de membrane sont aussi modélisées par un processus stochastique similaire, de «shot noise» filtré. Nous avons analysé les propriétés statistiques de ces processus dans le cadre des transformations de processus ponctuels de Poisson. Des propriétés de ces transformations sont dérivées les statistiques non-stationnaires du processus. Nous obtenons ainsi des expressions analytiques exactes pour les moments et cumulants du processus filtré dans le cas général des taux d'entrée variables. Ce travail ouvre de nombreuses perspectives pour l'analyse de neurones dans les conditions in vivo, en présence d'entrées synaptiques intenses et bruitées.","237955","Open Access","0.5548","-0.1896","11","Applications to neuronal, Dynamique stochastique, Filtered shot noise","Applications to neuronal, Dynamique stochastique, Filtered shot noise",NA,NA,"0","",""
"10.1088/1367-2630/15/2/023002","dedup_wf_001::7d43361ddb8d50631326e33e1aa19050","Quantitative Biology - Neurons and Cognition","Echoes in correlated neural systems","Helias, Moritz","2012-07-02","Dt. Physikalische Ges.","publication","","New Journal of Physics","","Correlations are employed in modern physics to explain microscopic and macroscopic phenomena, like the fractional quantum Hall effect and the Mott insulator state in high temperature superconductors and ultracold atoms. Simultaneously probed neurons in the intact brain reveal correlations between their activity, an important measure to study information processing in the brain that also influences macroscopic signals of neural activity, like the electro encephalogram (EEG). Networks of spiking neurons differ from most physical systems: The interaction between elements is directed, time delayed, mediated by short pulses, and each neuron receives events from thousands of neurons. Even the stationary state of the network cannot be described by equilibrium statistical mechanics. Here we develop a quantitative theory of pairwise correlations in finite sized random networks of spiking neurons. We derive explicit analytic expressions for the population averaged cross correlation functions. Our theory explains why the intuitive mean field description fails, how the echo of single action potentials causes an apparent lag of inhibition with respect to excitation, and how the size of the network can be scaled while maintaining its dynamical state. Finally, we derive a new criterion for the emergence of collective oscillations from the spectrum of the time-evolution propagator.","237955","Open Access","-0.2956","-0.3281","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","5","54","18","",""
"10.1103/physreve.91.062102","od________18::ed21635226dc7a254ab3313fa5336e56","Quantitative Biology - Neurons and Cognition","Non-stationary filtered shot noise processes and applications to neuronal membranes","Brigham, Marco","2014-10-22","","publication","","","","Filtered shot noise processes have proven to be very effective in modelling the evolution of systems exposed to stochastic shot noise sources, and have been applied to a wide variety of fields ranging from electronics through biology. In particular, they can model the membrane potential Vm of neurons driven by stochastic input, where these filtered processes are able to capture the non-stationary characteristics of Vm fluctuations in response to pre-synaptic input with variable rate. In this paper, we apply the general framework of Poisson Point Processes transformations to analyse these systems in the general case of variable input rate. We obtain exact analytic expressions, and very accurate approximations, for the joint cumulants of filtered shot noise processes with multiplicative noise. These general results are then applied to a model of neuronal membranes subject to conductance shot noise with continuously variable rate of pre-synaptic spikes. We propose very effective approximations for the time evolution of Vm distribution and simple method to estimate the pre-synaptic rate from a small number of Vm traces. This work opens the perspective of obtaining analytic access to important statistical properties of conductance-based neuronal models such as the the first passage time.","237955","Open Access","0.4736","-0.1156","11","Applications to neuronal, Dynamique stochastique, Filtered shot noise","Applications to neuronal, Dynamique stochastique, Filtered shot noise","3","15","0","",""
"10.1109/mcse.2011.119","dedup_wf_001::d5780dfeddf4c6d4446fc078ffc542a4"," applications;applications gpu;computing neuroscience","Three applications of GPU computing in neuroscience","Pezoa, Javier Baladron","2012-01-01","IEEE COMPUTER SOC","publication","","Computing in Science and Engineering","","","237955","Closed Access","0.3968","-0.7083","14","Applications gpu, Computing neuroscience","Applications gpu, Computing neuroscience","3","21","9","",""
"10.1137/130918721","dedup_wf_001::ad114081507ed1f5d4a5d76ea9ca77b5","65R20, 37M20, 92B20","Continuation of localised coherent structures in nonlocal neural field equations","Rankin, James","2013-04-26","Society for Industrial and Applied Mathematics","publication","","SIAM JOURNAL ON SCIENTIFIC COMPUTING","http://eprints.nottingham.ac.uk/2327/1/rankin-avitabile-etal-SISC-2013-revision.pdf","We study localised activity patterns in neural field equations posed on the Euclidean plane; such models are commonly used to describe the coarse-grained activity of large ensembles of cortical neurons in a spatially continuous way. We employ matrix-free Newton-Krylov\ud solvers and perform numerical continuation of localised patterns directly on the integral form of the equation. This opens up the possibility to study systems whose synaptic kernel does not lead to an equivalent PDE formulation. We present a numerical bifurcation study of localised states and show that the proposed models support\ud patterns of activity with varying spatial extent through the\ud mechanism of homoclinic snaking. The regular organisation of these patterns is due to spatial interactions at a specific scale associated with the separation of excitation peaks in the chosen connectivity function. The results presented form a basis for the general study of localised cortical activity with inputs and, more specifically, for investigating the localised spread of orientation selective activity that has been observed in the primary visual cortex with local visual input.","EP/H05040X/1","Open Access","-0.6646","0.1073","4","37m20, 65r20, 92b20","37m20, 65r20, 92b20","1","2","15","",""
"10.1152/jn.00194.2015","dedup_wf_001::ecc6a6394eabcd16df52e43d93c54412","Quantitative Biology - Neurons and Cognition","Testing the Odds of Inherent versus Observed Over-dispersion in Neural Spike Counts Odds of Inherent versus Observed Over-dispersion","Taouali, Wahiba","2016-11-14","American Physiological Society","publication","","","https://hal.archives-ouvertes.fr/hal-01396311/document","International audience; The repeated presentation of an identical visual stimulus in the receptive field of a neuron may evoke different spiking patterns at each trial. Probabilistic methods are essential to understand the functional role of this variance within the neural activity. In that case, a Poisson process is the most common model of trial-to-trial variability. For a Poisson process, the variance of the spike count is constrained to be equal to the mean, irrespective of the duration of measurements. Numerous studies have shown that this relationship does not generally hold. Specifically, a majority of electrophysiological recordings show an \" over-dispersion \" effect: Responses that exhibit more inter-trial variability than expected from a Poisson process alone. A model that is particularly well suited to quantify over-dispersion is the Negative-Binomial distribution model. This model is well-studied and widely used but has only recently been applied to neuroscience. In this paper, we address three main issues. First, we describe how the Negative-Binomial distribution provides a model apt to account for overdispersed spike counts. Second, we quantify the significance of this model for any neurophysiological data by proposing a statistical test, which quantifies the odds that over-dispersion could be due to the limited number of repetitions (trials). We apply this test to three neurophysiological tests along the visual pathway. Finally, we compare the performance of this model to the Poisson model on a population decoding task. We show that the decoding accuracy is improved when accounting for over-dispersion, especially under the hypothesis of tuned over-dispersion.","269921","Open Access","0.2346","0.4474","12","Inherent versus observed, Observed over dispersion, Odds of inherent","Inherent versus observed, Observed over dispersion, Odds of inherent","3","17","2","",""
"10.1152/jn.00953.2009","dedup_wf_001::b980750ab132435c64ce6a7fd865d4cd","Articles","Neuronal Avalanches in Spontaneous Activity In Vivo","Hahn, Gerald","2010-07-14","American Physiological Society","publication","","JOURNAL OF NEUROPHYSIOLOGY","","Many complex systems give rise to events that are clustered in space and time, thereby establishing a correlation structure that is governed by power law statistics. In the cortex, such clusters of activity, called “neuronal avalanches,” were recently found in local field potentials (LFPs) of spontaneous activity in acute cortex slices, slice cultures, the developing cortex of the anesthetized rat, and premotor and motor cortex of awake monkeys. At present, it is unclear whether neuronal avalanches also exist in the spontaneous LFPs and spike activity in vivo in sensory areas of the mature brain. To address this question, we recorded spontaneous LFPs and extracellular spiking activity with multiple 4 × 4 microelectrode arrays (Michigan Probes) in area 17 of adult cats under anesthesia. A cluster of events was defined as a consecutive sequence of time bins Δt (1–32 ms), each containing at least one LFP event or spike anywhere on the array. LFP cluster sizes consistently distributed according to a power law with a slope largely above –1.5. In two thirds of the corresponding experiments, spike clusters also displayed a power law that displayed a slightly steeper slope of −1.8 and was destroyed by subsampling operations. The power law in spike clusters was accompanied with stronger temporal correlations between spiking activities of neurons that spanned longer time periods compared with spike clusters lacking power law statistics. The results suggest that spontaneous activity of the visual cortex under anesthesia has the properties of neuronal avalanches.","237955","Open Access","-0.6717","-0.1239","4","37m20, 65r20, 92b20","37m20, 65r20, 92b20",NA,NA,"73","",""
"10.1186/1471-2202-12-s1-p124","dedup_wf_001::d67b15a89d4ed1353ff575f26fbfa817","Neurosciences. Biological psychiatry. Neuropsychiatry","General conditions for spiking neurons and plasticity rules to perform independent component analysis","Brito, Carlos SN","2011-07-01","BioMed Central","publication","","BMC Neuroscience","","","237955","Open Access","0.4012","0.532","7","Independent component analysis, Conditions for spiking, General conditions","Independent component analysis, Conditions for spiking, General conditions","1","2","0","",""
"10.1186/1471-2202-12-s1-p189","dedup_wf_001::8f730c1fb3c1c5f8554b45fea4d840f5","Neurosciences. Biological psychiatry. Neuropsychiatry","An abstract model of the basal ganglia, reward learning and action selection","Berthet Pierre","2011-07-01","BioMed Central","publication","","BMC Neuroscience","","","237955","Open Access","0.207","0.7364","5","Basal ganglia, Action selection, Reward prediction error","Basal ganglia, Action selection, Reward prediction error",NA,NA,"0","",""
"10.1186/1471-2202-14-s1-p227","od_______908::9c8297c7b6896f2ececa2a1489bf2712","Poster Presentation","Effects of single neuron firing patterns on network dynamics","Padmanabhan, Ajith","2013-07-01","BioMed Central","publication","","BMC Neuroscience","","","237955","Open Access","0.211","-0.5492","6","Single neuron","Single neuron",NA,NA,"","",""
"10.1186/1471-2202-14-s1-p314","od_______908::2b3c03aa9146749f0ed897df38986768","Poster Presentation","Motion based prediction and development of response to an \"on the way\" stimulus","Khoei, Mina A","2013-07-01","BioMed Central","publication","","BMC Neuroscience","","","237955","Open Access","0.6773","-0.3874","1","Motion based prediction","Motion based prediction",NA,NA,"0","",""
"10.1186/1471-2202-16-s1-o2","dedup_wf_001::10bef0108d04157e2ea6428a6e1fa2f1","Oral Presentation","The high-conductance state enables neural sampling in networks of LIF neurons","Petrovici, Mihai A.","2016-01-05","BioMed Central; Springer","publication","","","http://archiv.ub.uni-heidelberg.de/volltextserver/20741/1/12868_2015_Article_3846.pdf","The apparent stochasticity of in-vivo neural circuits has long been hypothesized to represent a signature of ongoing stochastic inference in the brain. More recently, a theoretical framework for neural sampling has been proposed, which explains how sample-based inference can be performed by networks of spiking neurons. One particular requirement of this approach is that the neural response function closely follows a logistic curve. Analytical approaches to calculating neural response functions have been the subject of many theoretical studies. In order to make the problem tractable, particular assumptions regarding the neural or synaptic parameters are usually made. However, biologically significant activity regimes exist which are not covered by these approaches: Under strong synaptic bombardment, as is often the case in cortex, the neuron is shifted into a high-conductance state (HCS) characterized by a small membrane time constant. In this regime, synaptic time constants and refractory periods dominate membrane dynamics. The core idea of our approach is to separately consider two different \"modes\" of spiking dynamics: burst spiking and transient quiescence, in which the neuron does not spike for longer periods. We treat the former by propagating the PDF of the effective membrane potential from spike to spike within a burst, while using a diffusion approximation for the latter. We find that our prediction of the neural response function closely matches simulation data. Moreover, in the HCS scenario, we show that the neural response function becomes symmetric and can be well approximated by a logistic function, thereby providing the correct dynamics in order to perform neural sampling. We hereby provide not only a normative framework for Bayesian inference in cortex, but also powerful applications of low-power, accelerated neuromorphic systems to relevant machine learning tasks.","269921","Open Access","0.1514","-0.2353","13","Spiking neurons","Spiking neurons",NA,NA,"3","",""
"10.1186/2190-8567-2-10","dedup_wf_001::147d980622a0ca54b896c458baf51b7d","Quantitative Biology::Neurons and Cognition","Mean-field description and propagation of chaos in networks of Hodgkin-Huxley and FitzHugh-Nagumo neurons.","Baladron, Javier","2012-05-31","BioMed Central","publication","","The Journal of Mathematical Neuroscience","http://www.hal.inserm.fr/inserm-00732288/document","We derive the mean-field equations arising as the limit of a network of interacting spiking neurons, as the number of neurons goes to infinity. The neurons belong to a fixed number of populations and are represented either by the Hodgkin-Huxley model or by one of its simplified version, the FitzHugh-Nagumo model. The synapses between neurons are either electrical or chemical. The network is assumed to be fully connected. The maximum conductances vary randomly. Under the condition that all neurons’ initial conditions are drawn independently from the same law that depends only on the population they belong to, we prove that a propagation of chaos phenomenon takes place, namely that in the mean-field limit, any finite number of neurons become independent and, within each population, have the same probability distribution. This probability distribution is a solution of a set of implicit equations, either nonlinear stochastic differential equations resembling the McKean-Vlasov equations or non-local partial differential equations resembling the McKean-Vlasov-Fokker-Planck equations. We prove the well-posedness of the McKean-Vlasov equations, i.e. the existence and uniqueness of a solution. We also show the results of some numerical experiments that indicate that the mean-field equations are a good representation of the mean activity of a finite size network, even for modest sizes. These experiments also indicate that the McKean-Vlasov-Fokker-Planck equations may be a good way to understand the mean-field dynamics through, e.g. a bifurcation analysis. Mathematics Subject Classification (2000): 60F99, 60B10, 92B20, 82C32, 82C80, 35Q80.","227747","Open Access","-0.4404","-0.4184","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks",NA,NA,"27","",""
"10.1186/s12868-016-0283-6.","dedup_wf_001::bda10a3f0f99adfbb7e00a60f7c69d3c","610 Medical sciences Medicine","25th Annual Computational Neuroscience Meeting: CNS-2016","Sharpee, Tatyana O.","2016-08-01","BioMed Central","publication","","BMC Neuroscience","http://archiv.ub.uni-heidelberg.de/volltextserver/21803/1/12868_2016_Article_283.pdf","Table of contents A1 Functional advantages of cell-type heterogeneity in neural circuits Tatyana O. Sharpee A2 Mesoscopic modeling of propagating waves in visual cortex Alain Destexhe A3 Dynamics and biomarkers of mental disorders Mitsuo Kawato F1 Precise recruitment of spiking output at theta frequencies requires dendritic h-channels in multi-compartment models of oriens-lacunosum/moleculare hippocampal interneurons Vladislav Sekulić, Frances K. Skinner F2 Kernel methods in reconstruction of current sources from extracellular potentials for single cells and the whole brains Daniel K. Wójcik, Chaitanya Chintaluri, Dorottya Cserpán, Zoltán Somogyvári F3 The synchronized periods depend on intracellular transcriptional repression mechanisms in circadian clocks. Jae Kyoung Kim, Zachary P. Kilpatrick, Matthew R. Bennett, Kresimir Josić O1 Assessing irregularity and coordination of spiking-bursting rhythms in central pattern generators Irene Elices, David Arroyo, Rafael Levi, Francisco B. Rodriguez, Pablo Varona O2 Regulation of top-down processing by cortically-projecting parvalbumin positive neurons in basal forebrain Eunjin Hwang, Bowon Kim, Hio-Been Han, Tae Kim, James T. McKenna, Ritchie E. Brown, Robert W. McCarley, Jee Hyun Choi O3 Modeling auditory stream segregation, build-up and bistability James Rankin, Pamela Osborn Popp, John Rinzel O4 Strong competition between tonotopic neural ensembles explains pitch-related dynamics of auditory cortex evoked fields Alejandro Tabas, André Rupp, Emili Balaguer-Ballester O5 A simple model of retinal response to multi-electrode stimulation Matias I. Maturana, David B. Grayden, Shaun L. Cloherty, Tatiana Kameneva, Michael R. Ibbotson, Hamish Meffin O6 Noise correlations in V4 area correlate with behavioral performance in visual discrimination task Veronika Koren, Timm Lochmann, Valentin Dragoi, Klaus Obermayer O7 Input-location dependent gain modulation in cerebellar nucleus neurons Maria Psarrou, Maria Schilstra, Neil Davey, Benjamin Torben-Nielsen, Volker Steuber O8 Analytic solution of cable energy function for cortical axons and dendrites Huiwen Ju, Jiao Yu, Michael L. Hines, Liang Chen, Yuguo Yu O9 C. elegans interactome: interactive visualization of Caenorhabditis elegans worm neuronal network Jimin Kim, Will Leahy, Eli Shlizerman O10 Is the model any good? Objective criteria for computational neuroscience model selection Justas Birgiolas, Richard C. Gerkin, Sharon M. Crook O11 Cooperation and competition of gamma oscillation mechanisms Atthaphon Viriyopase, Raoul-Martin Memmesheimer, Stan Gielen O12 A discrete structure of the brain waves Yuri Dabaghian, Justin DeVito, Luca Perotti O13 Direction-specific silencing of the Drosophila gaze stabilization system Anmo J. Kim, Lisa M. Fenk, Cheng Lyu, Gaby Maimon O14 What does the fruit fly think about values? A model of olfactory associative learning Chang Zhao, Yves Widmer, Simon Sprecher,Walter Senn O15 Effects of ionic diffusion on power spectra of local field potentials (LFP) Geir Halnes, Tuomo Mäki-Marttunen, Daniel Keller, Klas H. Pettersen,Ole A. Andreassen, Gaute T. Einevoll O16 Large-scale cortical models towards understanding relationship between brain structure abnormalities and cognitive deficits Yasunori Yamada O17 Spatial coarse-graining the brain: origin of minicolumns Moira L. Steyn-Ross, D. Alistair Steyn-Ross O18 Modeling large-scale cortical networks with laminar structure Jorge F. Mejias, John D. Murray, Henry Kennedy, Xiao-Jing Wang O19 Information filtering by partial synchronous spikes in a neural population Alexandra Kruscha, Jan Grewe, Jan Benda, Benjamin Lindner O20 Decoding context-dependent olfactory valence in Drosophila Laurent Badel, Kazumi Ohta, Yoshiko Tsuchimoto, Hokto Kazama P1 Neural network as a scale-free network: the role of a hub B. Kahng P2 Hemodynamic responses to emotions and decisions using near-infrared spectroscopy optical imaging Nicoladie D. Tam P3 Phase space analysis of hemodynamic responses to intentional movement directions using functional near-infrared spectroscopy (fNIRS) optical imaging technique Nicoladie D.Tam, Luca Pollonini, George Zouridakis P4 Modeling jamming avoidance of weakly electric fish Jaehyun Soh, DaeEun Kim P5 Synergy and redundancy of retinal ganglion cells in prediction Minsu Yoo, S. E. Palmer P6 A neural field model with a third dimension representing cortical depth Viviana Culmone, Ingo Bojak P7 Network analysis of a probabilistic connectivity model of the Xenopus tadpole spinal cord Andrea Ferrario, Robert Merrison-Hort, Roman Borisyuk P8 The recognition dynamics in the brain Chang Sub Kim P9 Multivariate spike train analysis using a positive definite kernel Taro Tezuka P10 Synchronization of burst periods may govern slow brain dynamics during general anesthesia Pangyu Joo P11 The ionic basis of heterogeneity affects stochastic synchrony Young-Ah Rho, Shawn D. Burton, G. Bard Ermentrout, Jaeseung Jeong, Nathaniel N. Urban P12 Circular statistics of noise in spike trains with a periodic component Petr Marsalek P14 Representations of directions in EEG-BCI using Gaussian readouts Hoon-Hee Kim, Seok-hyun Moon, Do-won Lee, Sung-beom Lee, Ji-yong Lee, Jaeseung Jeong P15 Action selection and reinforcement learning in basal ganglia during reaching movements Yaroslav I. Molkov, Khaldoun Hamade, Wondimu Teka, William H. Barnett, Taegyo Kim, Sergey Markin, Ilya A. Rybak P17 Axon guidance: modeling axonal growth in T-Junction assay Csaba Forro, Harald Dermutz, László Demkó, János Vörös P19 Transient cell assembly networks encode persistent spatial memories Yuri Dabaghian, Andrey Babichev P20 Theory of population coupling and applications to describe high order correlations in large populations of interacting neurons Haiping Huang P21 Design of biologically-realistic simulations for motor control Sergio Verduzco-Flores P22 Towards understanding the functional impact of the behavioural variability of neurons Filipa Dos Santos, Peter Andras P23 Different oscillatory dynamics underlying gamma entrainment deficits in schizophrenia Christoph Metzner, Achim Schweikard, Bartosz Zurowski P24 Memory recall and spike frequency adaptation James P. Roach, Leonard M. Sander, Michal R. Zochowski P25 Stability of neural networks and memory consolidation preferentially occur near criticality Quinton M. Skilling, Nicolette Ognjanovski, Sara J. Aton, Michal Zochowski P26 Stochastic Oscillation in Self-Organized Critical States of Small Systems: Sensitive Resting State in Neural Systems Sheng-Jun Wang, Guang Ouyang, Jing Guang, Mingsha Zhang, K. Y. Michael Wong, Changsong Zhou P27 Neurofield: a C++ library for fast simulation of 2D neural field models Peter A. Robinson, Paula Sanz-Leon, Peter M. Drysdale, Felix Fung, Romesh G. Abeysuriya, Chris J. Rennie, Xuelong Zhao P28 Action-based grounding: Beyond encoding/decoding in neural code Yoonsuck Choe, Huei-Fang Yang P29 Neural computation in a dynamical system with multiple time scales Yuanyuan Mi, Xiaohan Lin, Si Wu P30 Maximum entropy models for 3D layouts of orientation selectivity Joscha Liedtke, Manuel Schottdorf, Fred Wolf P31 A behavioral assay for probing computations underlying curiosity in rodents Yoriko Yamamura, Jeffery R. Wickens P32 Using statistical sampling to balance error function contributions to optimization of conductance-based models Timothy Rumbell, Julia Ramsey, Amy Reyes, Danel Draguljić, Patrick R. Hof, Jennifer Luebke, Christina M. Weaver P33 Exploration and implementation of a self-growing and self-organizing neuron network building algorithm Hu He, Xu Yang, Hailin Ma, Zhiheng Xu, Yuzhe Wang P34 Disrupted resting state brain network in obese subjects: a data-driven graph theory analysis Kwangyeol Baek, Laurel S. Morris, Prantik Kundu, Valerie Voon P35 Dynamics of cooperative excitatory and inhibitory plasticity Everton J. Agnes, Tim P. Vogels P36 Frequency-dependent oscillatory signal gating in feed-forward networks of integrate-and-fire neurons William F. Podlaski, Tim P. Vogels P37 Phenomenological neural model for adaptation of neurons in area IT Martin Giese, Pradeep Kuravi, Rufin Vogels P38 ICGenealogy: towards a common topology of neuronal ion channel function and genealogy in model and experiment Alexander Seeholzer, William Podlaski, Rajnish Ranjan, Tim Vogels P39 Temporal input discrimination from the interaction between dynamic synapses and neural subthreshold oscillations Joaquin J. Torres, Fabiano Baroni, Roberto Latorre, Pablo Varona P40 Different roles for transient and sustained activity during active visual processing Bart Gips, Eric Lowet, Mark J. Roberts, Peter de Weerd, Ole Jensen, Jan van der Eerden P41 Scale-free functional networks of 2D Ising model are highly robust against structural defects: neuroscience implications Abdorreza Goodarzinick, Mohammad D. Niry, Alireza Valizadeh P42 High frequency neuron can facilitate propagation of signal in neural networks Aref Pariz, Shervin S. Parsi, Alireza Valizadeh P43 Investigating the effect of Alzheimer’s disease related amyloidopathy on gamma oscillations in the CA1 region of the hippocampus Julia M. Warburton, Lucia Marucci, Francesco Tamagnini, Jon Brown, Krasimira Tsaneva-Atanasova P44 Long-tailed distributions of inhibitory and excitatory weights in a balanced network with eSTDP and iSTDP Florence I. Kleberg, Jochen Triesch P45 Simulation of EMG recording from hand muscle due to TMS of motor cortex Bahar Moezzi, Nicolangelo Iannella, Natalie Schaworonkow, Lukas Plogmacher, Mitchell R. Goldsworthy, Brenton Hordacre, Mark D. McDonnell, Michael C. Ridding, Jochen Triesch P46 Structure and dynamics of axon network formed in primary cell culture Martin Zapotocky, Daniel Smit, Coralie Fouquet, Alain Trembleau P47 Efficient signal processing and sampling in random networks that generate variability Sakyasingha Dasgupta, Isao Nishikawa, Kazuyuki Aihara, Taro Toyoizumi P48 Modeling the effect of riluzole on bursting in respiratory neural networks Daniel T. Robb, Nick Mellen, Natalia Toporikova P49 Mapping relaxation training using effective connectivity analysis Rongxiang Tang, Yi-Yuan Tang P50 Modeling neuron oscillation of implicit sequence learning Guangsheng Liang, Seth A. Kiser, James H. Howard, Jr., Yi-Yuan Tang P51 The role of cerebellar short-term synaptic plasticity in the pathology and medication of downbeat nystagmus Julia Goncharenko, Neil Davey, Maria Schilstra, Volker Steuber P52 Nonlinear response of noisy neurons Sergej O. Voronenko, Benjamin Lindner P53 Behavioral embedding suggests multiple chaotic dimensions underlie C. elegans locomotion Tosif Ahamed, Greg Stephens P54 Fast and scalable spike sorting for large and dense multi-electrodes recordings Pierre Yger, Baptiste Lefebvre, Giulia Lia Beatrice Spampinato, Elric Esposito, Marcel Stimberg et Olivier Marre P55 Sufficient sampling rates for fast hand motion tracking Hansol Choi, Min-Ho Song P56 Linear readout of object manifolds SueYeon Chung, Dan D. Lee, Haim Sompolinsky P57 Differentiating models of intrinsic bursting and rhythm generation of the respiratory pre-Bötzinger complex using phase response curves Ryan S. Phillips, Jeffrey Smith P58 The effect of inhibitory cell network interactions during theta rhythms on extracellular field potentials in CA1 hippocampus Alexandra Pierri Chatzikalymniou, Katie Ferguson, Frances K. Skinner P59 Expansion recoding through sparse sampling in the cerebellar input layer speeds learning N. Alex Cayco Gajic, Claudia Clopath, R. Angus Silver P60 A set of curated cortical models at multiple scales on Open Source Brain Padraig Gleeson, Boris Marin, Sadra Sadeh, Adrian Quintana, Matteo Cantarelli, Salvador Dura-Bernal, William W. Lytton, Andrew Davison, R. Angus Silver P61 A synaptic story of dynamical information encoding in neural adaptation Luozheng Li, Wenhao Zhang, Yuanyuan Mi, Dahui Wang, Si Wu P62 Physical modeling of rule-observant rodent behavior Youngjo Song, Sol Park, Ilhwan Choi, Jaeseung Jeong, Hee-sup Shin P64 Predictive coding in area V4 and prefrontal cortex explains dynamic discrimination of partially occluded shapes Hannah Choi, Anitha Pasupathy, Eric Shea-Brown P65 Stability of FORCE learning on spiking and rate-based networks Dongsung Huh, Terrence J. Sejnowski P66 Stabilising STDP in striatal neurons for reliable fast state recognition in noisy environments Simon M. Vogt, Arvind Kumar, Robert Schmidt P67 Electrodiffusion in one- and two-compartment neuron models for characterizing cellular effects of electrical stimulation Stephen Van Wert, Steven J. Schiff P68 STDP improves speech recognition capabilities in spiking recurrent circuits parameterized via differential evolution Markov Chain Monte Carlo Richard Veale, Matthias Scheutz P69 Bidirectional transformation between dominant cortical neural activities and phase difference distributions Sang Wan Lee P70 Maturation of sensory networks through homeostatic structural plasticity Júlia Gallinaro, Stefan Rotter P71 Corticothalamic dynamics: structure, number of solutions and stability of steady-state solutions in the space of synaptic couplings Paula Sanz-Leon, Peter A. Robinson P72 Optogenetic versus electrical stimulation of the parkinsonian basal ganglia. Computational study Leonid L. Rubchinsky, Chung Ching Cheung, Shivakeshavan Ratnadurai-Giridharan P73 Exact spike-timing distribution reveals higher-order interactions of neurons Safura Rashid Shomali, Majid Nili Ahmadabadi, Hideaki Shimazaki, S. Nader Rasuli P74 Neural mechanism of visual perceptual learning using a multi-layered neural network Xiaochen Zhao, Malte J. Rasch P75 Inferring collective spiking dynamics from mostly unobserved systems Jens Wilting, Viola Priesemann P76 How to infer distributions in the brain from subsampled observations Anna Levina, Viola Priesemann P77 Influences of embedding and estimation strategies on the inferred memory of single spiking neurons Lucas Rudelt, Joseph T. Lizier, Viola Priesemann P78 A nearest-neighbours based estimator for transfer entropy between spike trains Joseph T. Lizier, Richard E. Spinney, Mikail Rubinov, Michael Wibral, Viola Priesemann P79 Active learning of psychometric functions with multinomial logistic models Ji Hyun Bak, Jonathan Pillow P81 Inferring low-dimensional network dynamics with variational latent Gaussian process Yuan Zaho, Il Memming Park P82 Computational investigation of energy landscapes in the resting state subcortical brain network Jiyoung Kang, Hae-Jeong Park P83 Local repulsive interaction between retinal ganglion cells can generate a consistent spatial periodicity of orientation map Jaeson Jang, Se-Bum Paik P84 Phase duration of bistable perception reveals intrinsic time scale of perceptual decision under noisy condition Woochul Choi, Se-Bum Paik P85 Feedforward convergence between retina and primary visual cortex can determine the structure of orientation map Changju Lee, Jaeson Jang, Se-Bum Paik P86 Computational method classifying neural network activity patterns for imaging data Min Song, Hyeonsu Lee, Se-Bum Paik P87 Symmetry of spike-timing-dependent-plasticity kernels regulates volatility of memory Youngjin Park, Woochul Choi, Se-Bum Paik P88 Effects of time-periodic coupling strength on the first-spike latency dynamics of a scale-free network of stochastic Hodgkin-Huxley neurons Ergin Yilmaz, Veli Baysal, Mahmut Ozer P89 Spectral properties of spiking responses in V1 and V4 change within the trial and are highly relevant for behavioral performance Veronika Koren, Klaus Obermayer P90 Methods for building accurate models of individual neurons Daniel Saska, Thomas Nowotny P91 A full size mathematical model of the early olfactory system of honeybees Ho Ka Chan, Alan Diamond, Thomas Nowotny P92 Stimulation-induced tuning of ongoing oscillations in spiking neural networks Christoph S. Herrmann, Micah M. Murray, Silvio Ionta, Axel Hutt, Jérémie Lefebvre P93 Decision-specific sequences of neural activity in balanced random networks driven by structured sensory input Philipp Weidel, Renato Duarte, Abigail Morrison P94 Modulation of tuning induced by abrupt reduction of SST cell activity Jung H. Lee, Ramakrishnan Iyer, Stefan Mihalas P95 The functional role of VIP cell activation during locomotion Jung H. Lee, Ramakrishnan Iyer, Christof Koch, Stefan Mihalas P96 Stochastic inference with spiking neural networks Mihai A. Petrovici, Luziwei Leng, Oliver Breitwieser, David Stöckel, Ilja Bytschok, Roman Martel, Johannes Bill, Johannes Schemmel, Karlheinz Meier P97 Modeling orientation-selective electrical stimulation with retinal prostheses Timothy B. Esler, Anthony N. Burkitt, David B. Grayden, Robert R. Kerr, Bahman Tahayori, Hamish Meffin P98 Ion channel noise can explain firing correlation in auditory nerves Bahar Moezzi, Nicolangelo Iannella, Mark D. McDonnell P99 Limits of temporal encoding of thalamocortical inputs in a neocortical microcircuit Max Nolte, Michael W. Reimann, Eilif Muller, Henry Markram P100 On the representation of arm reaching movements: a computational model Antonio Parziale, Rosa Senatore, Angelo Marcelli P101 A computational model for investigating the role of cerebellum in acquisition and retention of motor behavior Rosa Senatore, Antonio Parziale, Angelo Marcelli P102 The emergence of semantic categories from a large-scale brain network of semantic knowledge K. Skiker, M. Maouene P103 Multiscale modeling of M1 multitarget pharmacotherapy for dystonia Samuel A. Neymotin, Salvador Dura-Bernal, Alexandra Seidenstein, Peter Lakatos, Terence D. Sanger, William W. Lytton P104 Effect of network size on computational capacity Salvador Dura-Bernal, Rosemary J. Menzies, Campbell McLauchlan, Sacha J. van Albada, David J. Kedziora, Samuel Neymotin, William W. Lytton, Cliff C. Kerr P105 NetPyNE: a Python package for NEURON to facilitate development and parallel simulation of biological neuronal networks Salvador Dura-Bernal, Benjamin A. Suter, Samuel A. Neymotin, Cliff C. Kerr, Adrian Quintana, Padraig Gleeson, Gordon M. G. Shepherd, William W. Lytton P107 Inter-areal and inter-regional inhomogeneity in co-axial anisotropy of Cortical Point Spread in human visual areas Juhyoung Ryu, Sang-Hun Lee P108 Two bayesian quanta of uncertainty explain the temporal dynamics of cortical activity in the non-sensory areas during bistable perception Joonwon Lee, Sang-Hun Lee P109 Optimal and suboptimal integration of sensory and value information in perceptual decision making Hyang Jung Lee, Sang-Hun Lee P110 A Bayesian algorithm for phoneme Perception and its neural implementation Daeseob Lim, Sang-Hun Lee P111 Complexity of EEG signals is reduced during unconsciousness induced by ketamine and propofol Jisung Wang, Heonsoo Lee P112 Self-organized criticality of neural avalanche in a neural model on complex networks Nam Jung, Le Anh Quang, Seung Eun Maeng, Tae Ho Lee, Jae Woo Lee P113 Dynamic alterations in connection topology of the hippocampal network during ictal-like epileptiform activity in an in vitro rat model Chang-hyun Park, Sora Ahn, Jangsup Moon, Yun Seo Choi, Juhee Kim, Sang Beom Jun, Seungjun Lee, Hyang Woon Lee P114 Computational model to replicate seizure suppression effect by electrical stimulation Sora Ahn, Sumin Jo, Eunji Jun, Suin Yu, Hyang Woon Lee, Sang Beom Jun, Seungjun Lee P115 Identifying excitatory and inhibitory synapses in neuronal networks from spike trains using sorted local transfer entropy Felix Goetze, Pik-Yin Lai P116 Neural network model for obstacle avoidance based on neuromorphic computational model of boundary vector cell and head direction cell Seonghyun Kim, Jeehyun Kwag P117 Dynamic gating of spike pattern propagation by Hebbian and anti-Hebbian spike timing-dependent plasticity in excitatory feedforward network model Hyun Jae Jang, Jeehyun Kwag P118 Inferring characteristics of input correlations of cells exhibiting up-down state transitions in the rat striatum Marko Filipović, Ramon Reig, Ad Aertsen, Gilad Silberberg, Arvind Kumar P119 Graph properties of the functional connected brain under the influence of Alzheimer’s disease Claudia Bachmann, Simone Buttler, Heidi Jacobs, Kim Dillen, Gereon R. Fink, Juraj Kukolja, Abigail Morrison P120 Learning sparse representations in the olfactory bulb Daniel Kepple, Hamza Giaffar, Dima Rinberg, Steven Shea, Alex Koulakov P121 Functional classification of homologous basal-ganglia networks Jyotika Bahuguna,Tom Tetzlaff, Abigail Morrison, Arvind Kumar, Jeanette Hellgren Kotaleski P122 Short term memory based on multistability Tim Kunze, Andre Peterson, Thomas Knösche P123 A physiologically plausible, computationally efficient model and simulation software for mammalian motor units Minjung Kim, Hojeong Kim P125 Decoding laser-induced somatosensory information from EEG Ji Sung Park, Ji Won Yeon, Sung-Phil Kim P126 Phase synchronization of alpha activity for EEG-based personal authentication Jae-Hwan Kang, Chungho Lee, Sung-Phil Kim P129 Investigating phase-lags in sEEG data using spatially distributed time delays in a large-scale brain network model Andreas Spiegler, Spase Petkoski, Matias J. Palva, Viktor K. Jirsa P130 Epileptic seizures in the unfolding of a codimension-3 singularity Maria L. Saggio, Silvan F. Siep, Andreas Spiegler, William C. Stacey, Christophe Bernard, Viktor K. Jirsa P131 Incremental dimensional exploratory reasoning under multi-dimensional environment Oh-hyeon Choung, Yong Jeong P132 A low-cost model of eye movements and memory in personal visual cognition Yong-il Lee, Jaeseung Jeong P133 Complex network analysis of structural connectome of autism spectrum disorder patients Su Hyun Kim, Mir Jeong, Jaeseung Jeong P134 Cognitive motives and the neural correlates underlying human social information transmission, gossip Jeungmin Lee, Jaehyung Kwon, Jerald D. Kralik, Jaeseung Jeong P135 EEG hyperscanning detects neural oscillation for the social interaction during the economic decision-making Jaehwan Jahng, Dong-Uk Hwang, Jaeseung Jeong P136 Detecting purchase decision based on hyperfrontality of the EEG Jae-Hyung Kwon, Sang-Min Park, Jaeseung Jeong P137 Vulnerability-based critical neurons, synapses, and pathways in the Caenorhabditis elegans connectome Seongkyun Kim, Hyoungkyu Kim, Jerald D. Kralik, Jaeseung Jeong P138 Motif analysis reveals functionally asymmetrical neurons in C. elegans Pyeong Soo Kim, Seongkyun Kim, Hyoungkyu Kim, Jaeseung Jeong P139 Computational approach to preference-based serial decision dynamics: do temporal discounting and working memory affect it? Sangsup Yoon, Jaehyung Kwon, Sewoong Lim, Jaeseung Jeong P141 Social stress induced neural network reconfiguration affects decision making and learning in zebrafish Choongseok Park, Thomas Miller, Katie Clements, Sungwoo Ahn, Eoon Hye Ji, Fadi A. Issa P142 Descriptive, generative, and hybrid approaches for neural connectivity inference from neural activity data JeongHun Baek, Shigeyuki Oba, Junichiro Yoshimoto, Kenji Doya, Shin Ishii P145 Divergent-convergent synaptic connectivities accelerate coding in multilayered sensory systems Thiago S. Mosqueiro, Martin F. Strube-Bloss, Brian Smith, Ramon Huerta P146 Swinging networks Michal Hadrava, Jaroslav Hlinka P147 Inferring dynamically relevant motifs from oscillatory stimuli: challenges, pitfalls, and solutions Hannah Bos, Moritz Helias P148 Spatiotemporal mapping of brain network dynamics during cognitive tasks using magnetoencephalography and deep learning Charles M. Welzig, Zachary J. Harper P149 Multiscale complexity analysis for the segmentation of MRI images Won Sup Kim, In-Seob Shin, Hyeon-Man Baek, Seung Kee Han P150 A neuro-computational model of emotional attention René Richter, Julien Vitay, Frederick Beuth, Fred H. Hamker P151 Multi-site delayed feedback stimulation in parkinsonian networks Kelly Toppin, Yixin Guo P152 Bistability in Hodgkin–Huxley-type equations Tatiana Kameneva, Hamish Meffin, Anthony N. Burkitt, David B. Grayden P153 Phase changes in postsynaptic spiking due to synaptic connectivity and short term plasticity: mathematical analysis of frequency dependency Mark D. McDonnell, Bruce P. Graham P154 Quantifying resilience patterns in brain networks: the importance of directionality Penelope J. Kale, Leonardo L. Gollo P155 Dynamics of rate-model networks with separate excitatory and inhibitory populations Merav Stern, L. F. Abbott P156 A model for multi-stable dynamics in action recognition modulated by integration of silhouette and shading cues Leonid A. Fedorov, Martin A. Giese P157 Spiking model for the interaction between action recognition and action execution Mohammad Hovaidi Ardestani, Martin Giese P158 Surprise-modulated belief update: how to learn within changing environments? Mohammad Javad Faraji, Kerstin Preuschoff, Wulfram Gerstner P159 A fast, stochastic and adaptive model of auditory nerve responses to cochlear implant stimulation Margriet J. van Gendt, Jeroen J. Briaire, Randy K. Kalkman, Johan H. M. Frijns P160 Quantitative comparison of graph theoretical measures of simulated and empirical functional brain networks Won Hee Lee, Sophia Frangou P161 Determining discriminative properties of fMRI signals in schizophrenia using highly comparative time-series analysis Ben D. Fulcher, Patricia H. P. Tran, Alex Fornito P162 Emergence of narrowband LFP oscillations from completely asynchronous activity during seizures and high-frequency oscillations Stephen V. Gliske, William C. Stacey, Eugene Lim, Katherine A. Holman, Christian G. Fink P163 Neuronal diversity in structure and function: cross-validation of anatomical and physiological classification of retinal ganglion cells in the mouse Jinseop S. Kim, Shang Mu, Kevin L. Briggman, H. Sebastian Seung, the EyeWirers P164 Analysis and modelling of transient firing rate changes in area MT in response to rapid stimulus feature changes Detlef Wegener, Lisa Bohnenkamp, Udo A. Ernst P165 Step-wise model fitting accounting for high-resolution spatial measurements: construction of a layer V pyramidal cell model with reduced morphology Tuomo Mäki-Marttunen, Geir Halnes, Anna Devor, Christoph Metzner, Anders M. Dale, Ole A. Andreassen, Gaute T. Einevoll P166 Contributions of schizophrenia-associated genes to neuron firing and cardiac pacemaking: a polygenic modeling approach Tuomo Mäki-Marttunen, Glenn T. Lines, Andy Edwards, Aslak Tveito, Anders M. Dale, Gaute T. Einevoll, Ole A. Andreassen P167 Local field potentials in a 4 × 4 mm2 multi-layered network model Espen Hagen, Johanna Senk, Sacha J. van Albada, Markus Diesmann P168 A spiking network model explains multi-scale properties of cortical dynamics Maximilian Schmidt, Rembrandt Bakker, Kelly Shen, Gleb Bezgin, Claus-Christian Hilgetag, Markus Diesmann, Sacha Jennifer van Albada P169 Using joint weight-delay spike-timing dependent plasticity to find polychronous neuronal groups Haoqi Sun, Olga Sourina, Guang-Bin Huang, Felix Klanner, Cornelia Denk P170 Tensor decomposition reveals RSNs in simulated resting state fMRI Katharina Glomb, Adrián Ponce-Alvarez, Matthieu Gilson, Petra Ritter, Gustavo Deco P171 Getting in the groove: testing a new model-based method for comparing task-evoked vs resting-state activity in fMRI data on music listening Matthieu Gilson, Maria AG Witek, Eric F. Clarke, Mads Hansen, Mikkel Wallentin, Gustavo Deco, Morten L. Kringelbach, Peter Vuust P172 STochastic engine for pathway simulation (STEPS) on massively parallel processors Guido Klingbeil, Erik De Schutter P173 Toolkit support for complex parallel spatial stochastic reaction–diffusion simulation in STEPS Weiliang Chen, Erik De Schutter P174 Modeling the generation and propagation of Purkinje cell dendritic spikes caused by parallel fiber synaptic input Yunliang Zang, Erik De Schutter P175 Dendritic morphology determines how dendrites are organized into functional subunits Sungho Hong, Akira Takashima, Erik De Schutter P176 A model of Ca2+/calmodulin-dependent protein kinase II activity in long term depression at Purkinje cells Criseida Zamora, Andrew R. Gallimore, Erik De Schutter P177 Reward-modulated learning of population-encoded vectors for insect-like navigation in embodied agents Dennis Goldschmidt, Poramate Manoonpong, Sakyasingha Dasgupta P178 Data-driven neural models part II: connectivity patterns of human seizures Philippa J. Karoly, Dean R. Freestone, Daniel Soundry, Levin Kuhlmann, Liam Paninski, Mark Cook P179 Data-driven neural models part I: state and parameter estimation Dean R. Freestone, Philippa J. Karoly, Daniel Soundry, Levin Kuhlmann, Mark Cook P180 Spectral and spatial information processing in human auditory streaming Jaejin Lee, Yonatan I. Fishman, Yale E. Cohen P181 A tuning curve for the global effects of local perturbations in neural activity: Mapping the systems-level susceptibility of the brain Leonardo L. Gollo, James A. Roberts, Luca Cocchi P182 Diverse homeostatic responses to visual deprivation mediated by neural ensembles Yann Sweeney, Claudia Clopath P183 Opto-EEG: a novel method for investigating functional connectome in mouse brain based on optogenetics and high density electroencephalography Soohyun Lee, Woo-Sung Jung, Jee Hyun Choi P184 Biphasic responses of frontal gamma network to repetitive sleep deprivation during REM sleep Bowon Kim, Youngsoo Kim, Eunjin Hwang, Jee Hyun Choi P185 Brain-state correlate and cortical connectivity for frontal gamma oscillations in top-down fashion assessed by auditory steady-state response Younginha Jung, Eunjin Hwang, Yoon-Kyu Song, Jee Hyun Choi P186 Neural field model of localized orientation selective activation in V1 James Rankin, Frédéric Chavane P187 An oscillatory network model of Head direction and Grid cells using locomotor inputs Karthik Soman, Vignesh Muralidharan, V. Srinivasa Chakravarthy P188 A computational model of hippocampus inspired by the functional architecture of basal ganglia Karthik Soman, Vignesh Muralidharan, V. Srinivasa Chakravarthy P189 A computational architecture to model the microanatomy of the striatum and its functional properties Sabyasachi Shivkumar, Vignesh Muralidharan, V. Srinivasa Chakravarthy P190 A scalable cortico-basal ganglia model to understand the neural dynamics of targeted reaching Vignesh Muralidharan, Alekhya Mandali, B. Pragathi Priyadharsini, Hima Mehta, V. Srinivasa Chakravarthy P191 Emergence of radial orientation selectivity from synaptic plasticity Catherine E. Davey, David B. Grayden, Anthony N. Burkitt P192 How do hidden units shape effective connections between neurons? Braden A. W. Brinkman, Tyler Kekona, Fred Rieke, Eric Shea-Brown, Michael Buice P193 Characterization of neural firing in the presence of astrocyte-synapse signaling Maurizio De Pittà, Hugues Berry, Nicolas Brunel P194 Metastability of spatiotemporal patterns in a large-scale network model of brain dynamics James A. Roberts, Leonardo L. Gollo, Michael Breakspear P195 Comparison of three methods to quantify detection and discrimination capacity estimated from neural population recordings Gary Marsat, Jordan Drew, Phillip D. Chapman, Kevin C. Daly, Samual P. Bradley P196 Quantifying the constraints for independent evoked and spontaneous NMDA receptor mediated synaptic transmission at individual synapses Sat Byul Seo, Jianzhong Su, Ege T. Kavalali, Justin Blackwell P199 Gamma oscillation via adaptive exponential integrate-and-fire neurons LieJune Shiau, Laure Buhry, Kanishka Basnayake P200 Visual face representations during memory retrieval compared to perception Sue-Hyun Lee, Brandon A. Levy, Chris I. Baker P201 Top-down modulation of sequential activity within packets modeled using avalanche dynamics Timothée Leleu, Kazuyuki Aihara Q28 An auto-encoder network realizes sparse features under the influence of desynchronized vascular dynamics Ryan T. Philips, Karishma Chhabria, V. Srinivasa Chakravarthy","604102","Open Access","-0.1307","0.077","13","Spiking neurons","Spiking neurons",NA,NA,"","",""
"10.1186/s13408-015-0020-y","dedup_wf_001::60bb4e4f9f4bdd3ca8e0cf291a14f596","Perturbative theory","A Formalism for Evaluating Analytically the Cross-Correlation Structure of a Firing-Rate Network Model","Fasoli, Diego","2015-03-01","Springer Berlin Heidelberg","publication","","Journal of Mathematical Neuroscience","https://hal.inria.fr/hal-01208576/document","We introduce a new formalism for evaluating analytically the cross-correlation structure of a finite-size firing-rate network with recurrent connections. The analysis performs a first-order perturbative expansion of neural activity equations that include three different sources of randomness: the background noise of the membrane potentials, their initial conditions, and the distribution of the recurrent synaptic weights. This allows the analytical quantification of the relationship between anatomical and functional connectivity, i.e. of how the synaptic connections determine the statistical dependencies at any order among different neurons. The technique we develop is general, but for simplicity and clarity we demonstrate its efficacy by applying it to the case of synaptic connections described by regular graphs. The analytical equations so obtained reveal previously unknown behaviors of recurrent firing-rate networks, especially on how correlations are modified by the external input, by the finite size of the network, by the density of the anatomical connections and by correlation in sources of randomness. In particular, we show that a strong input can make the neurons almost independent, suggesting that functional connectivity does not depend only on the static anatomical connectivity, but also on the external inputs. Moreover we prove that in general it is not possible to find a mean-field description ? la Sznitman of the network, if the anatomical connections are too sparse or our three sources of variability are correlated. To conclude, we show a very counterintuitive phenomenon, which we call stochastic synchronization, through which neurons become almost perfectly correlated even if the sources of randomness are independent. Due to its ability to quantify how activity of individual neurons and the correlation among them depends upon external inputs, the formalism introduced here can serve as a basis for exploring analytically the computational capability of population codes expressed by recurrent neural networks. Electronic Supplementary Material The online version of this article (doi:10.1186/s13408-015-0020-y) contains supplementary material 1.","227747","Open Access","-0.2289","-0.4694","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","1","16","3","",""
"10.1371/journal.pcbi.1002059","od________18::7e1902e58c996d30317679aba0bd2bca","Quantitative Biology - Neurons and Cognition","Finite size effects in the correlation structure of stochastic neural networks: analysis of different connectivity matrices and failure of the mean-field theory","Fasoli, D.","2013-07-08","","publication","","","","We quantify the finite size effects in a stochastic network made up of rate neurons, for several kinds of recurrent connectivity matrices. This analysis is performed by means of a perturbative expansion of the neural equations, where the perturbative parameters are the intensities of the sources of randomness in the system. In detail, these parameters are the variances of the background or input noise, of the initial conditions and of the distribution of the synaptic weights. The technique developed in this article can be used to study systems which are invariant under the exchange of the neural indices and it allows us to quantify the correlation structure of the network, in terms of pairwise and higher order correlations between the neurons. We also determine the relation between the correlation and the external input of the network, showing that strong signals coming from the environment reduce significantly the amount of correlation between the neurons. Moreover we prove that in general the phenomenon of propagation of chaos does not occur, even in the thermodynamic limit, due to the correlation structure of the 3 sources of randomness considered in the model. Furthermore, we show that the propagation of chaos does not depend only on the number of neurons in the network, but also and mainly on the number of incoming connections per neuron. To conclude, we prove that for special values of the parameters of the system the neurons become perfectly correlated, a phenomenon that we have called stochastic synchronization. These discoveries clearly prevent the use of the mean-field theory in the description of the neural network.","237955","Open Access","-0.1435","-0.3824","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks",NA,NA,"83","",""
"10.1371/journal.pcbi.1002059","od________18::af743cb1651ee8869ce77dfa120618a7","Quantitative Biology - Neurons and Cognition","Correlation structure of stochastic neural networks with generic connectivity matrices","Fasoli, D.","2013-07-10","","publication","","","","Using a perturbative expansion for weak synaptic weights and weak sources of randomness, we calculate the correlation structure of neural networks with generic connectivity matrices. In detail, the perturbative parameters are the mean and the standard deviation of the synaptic weights, together with the standard deviations of the background noise of the membrane potentials and of their initial conditions. We also show how to determine the correlation structure of the system when the synaptic connections have a random topology. This analysis is performed on rate neurons described by Wilson and Cowan equations, since this allows us to find analytic results. Moreover, the perturbative expansion can be developed at any order and for a generic connectivity matrix. We finally show an example of application of this technique for a particular case of biologically relevant topology of the synaptic connections.","269921","Open Access","-0.1422","-0.5621","10","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks","Correlation structure, Quantitative biology - neurons and cognition, Balanced networks",NA,NA,"83","",""
"10.1371/journal.pcbi.1002211","dedup_wf_001::da16cc4bcf28762ebfb6dfa35e39c330","Research Article","Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons","Buesing, Lars","2011-01-01","PUBLIC LIBRARY SCIENCE","publication","","PLOS COMPUTATIONAL BIOLOGY","","The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.","269921","Open Access","-0.1028","-0.0387","13","Spiking neurons","Spiking neurons","16","345","90","",""
"10.1371/journal.pcbi.1003330","dedup_wf_001::bd64087b560fa69842a474cd57a65a97","Research Article","Synaptic Plasticity in Neural Networks Needs Homeostasis with a Fast Rate Detector","Zenke, Friedemann","2013-11-01","Public Library of Science","publication","","PLoS Computational Biology","","Author Summary Learning and memory in the brain are thought to be mediated through Hebbian plasticity. When a group of neurons is repetitively active together, their connections get strengthened. This can cause co-activation even in the absence of the stimulus that triggered the change. To avoid run-away behavior it is important to prevent neurons from forming excessively strong connections. This is achieved by regulatory homeostatic mechanisms that constrain the overall activity. Here we study the stability of background activity in a recurrent network model with a plausible Hebbian learning rule and homeostasis. We find that the activity in our model is unstable unless homeostasis reacts to rate changes on a timescale of minutes or faster. Since this timescale is incompatible with most known forms of homeostasis, this implies the existence of a previously unknown, rapid homeostatic regulatory mechanism capable of either gating the rate of plasticity, or affecting synaptic efficacies otherwise on a short timescale.","269921","Open Access","-0.5434","0.277","15","Synaptic plasticity, Neural networks","Synaptic plasticity, Neural networks","6","152","46","",""
"10.1371/journal.pcbi.1003811","dedup_wf_001::5b392f195190f30dbc94b017440a2d03","Computational Biology","Communication through Resonance in Spiking Neuronal Networks","Hahn, Gerald","2014-08-01","Public Library of Science","publication","","PLoS Computational Biology","","Author Summary The cortex is a highly modular structure with a large number of functionally specialized areas that communicate with each other through long-range cortical connections. It is has been suggested that communication between spiking neuronal networks (SNNs) requires synchronization of spiking activity which is either provided by the flow of neuronal activity across divergent/convergent connections, as suggested by computational models of SNNs, or by local oscillations in the gamma frequency band (30–100 Hz). However, such communication requires unphysiologically dense/strong connectivity, and the mechanisms required to synchronize separated local oscillators remain poorly understood. Here, we present a novel mechanism that alleviates these shortcomings and enables the propagation synchrony across weakly connected SNNs by locally amplifying feeble synchronization through resonance that naturally occurs in oscillating networks of excitatory and inhibitory neurons. We show that oscillatory stimuli at the network resonance frequencies generate a slowly propagating oscillation that is synchronized across the distributed networks. Moreover, communication with such oscillations depends on the dynamical state of the background activity in the SNN. Our results suggest that the emergence of synchronized oscillations can be viewed as a consequence of spiking activity propagation in weakly connected networks that is supported by resonance and modulated by the dynamics of the ongoing activity.","237955","Open Access","-0.4671","-0.2334","8","Network model, Spiking neural network","Network model, Spiking neural network","14","129","17","",""
"10.1371/journal.pcbi.1005068","dedup_wf_001::3efe1bbf96994a131c4e24f2e0ab2bac","Computational Biology","The Flash-Lag Effect as a Motion-Based Predictive Shift.","Mina A Khoei","2017-01-01","Public Library of Science (PLoS)","publication","","PLoS Computational Biology","","Due to its inherent neural delays, the visual system has an outdated access to sensory information about the current position of moving objects. In contrast, living organisms are remarkably able to track and intercept moving objects under a large range of challenging environmental conditions. Physiological, behavioral and psychophysical evidences strongly suggest that position coding is extrapolated using an explicit and reliable representation of object?s motion but it is still unclear how these two representations interact. For instance, the so-called flash-lag effect supports the idea of a differential processing of position between moving and static objects. Although elucidating such mechanisms is crucial in our understanding of the dynamics of visual processing, a theory is still missing to explain the different facets of this visual illusion. Here, we reconsider several of the key aspects of the flash-lag effect in order to explore the role of motion upon neural coding of objects? position. First, we formalize the problem using a Bayesian modeling framework which includes a graded representation of the degree of belief about visual motion. We introduce a motion-based prediction model as a candidate explanation for the perception of coherent motion. By including the knowledge of a fixed delay, we can model the dynamics of sensory information integration by extrapolating the information acquired at previous instants in time. Next, we simulate the optimal estimation of object position with and without delay compensation and compared it with human perception under a broad range of different psychophysical conditions. Our computational study suggests that the explicit, probabilistic representation of velocity information is crucial in explaining position coding, and therefore the flash-lag effect. We discuss these theoretical results in light of the putative corrective mechanisms that can be used to cancel out the detrimental effects of neural delays and illuminate the more general question of the dynamical representation at the present time of spatial information in the visual pathways.","unidentified","Open Access","-0.3394","0.6059","1","Motion based prediction","Motion based prediction","9","14","1","",""
"10.1371/journal.pcbi.1005070","dedup_wf_001::8d894913f2da21d0b7cd6a7413eec441","Computational Biology","Nonlinear Hebbian Learning as a Unifying Principle in Receptive Field Formation","Brito, Carlos S. N.","2016-09-01","Public Library of Science","publication","","PLoS Computational Biology","","The development of sensory receptive fields has been modeled in the past by a variety of models including normative models such as sparse coding or independent component analysis and bottom-up models such as spike-timing dependent plasticity or the Bienen-stock-Cooper-Munro model of synaptic plasticity. Here we show that the above variety of approaches can all be unified into a single common principle, namely nonlinear Hebbian learning. When nonlinear Hebbian learning is applied to natural images, receptive field shapes were strongly constrained by the input statistics and preprocessing, but exhibited only modest variation across different choices of nonlinearities in neuron models or synaptic plasticity rules. Neither overcompleteness nor sparse network activity are necessary for the development of localized receptive fields. The analysis of alternative sensory modalities such as auditory models or V2 development lead to the same conclusions. In all examples, receptive fields can be predicted a priori by reformulating an abstract model as nonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural statistics can account for many aspects of receptive field formation across models and sensory modalities.","268689","Open Access","-0.1772","0.5886","12","Inherent versus observed, Observed over dispersion, Odds of inherent","Inherent versus observed, Observed over dispersion, Odds of inherent","18","87","9","",""
"10.1371/journal.pcbi.1005543","dedup_wf_001::5d7e185410c8aa4fcec668ea24b7f794","Research Article","Spontaneous cortical activity is transiently poised close to criticality","Hahn, Gerald","2017-05-01","Public Library of Science","publication","","PLoS Computational Biology","","Brain activity displays a large repertoire of dynamics across the sleep-wake cycle and even during anesthesia. It was suggested that criticality could serve as a unifying principle underlying the diversity of dynamics. This view has been supported by the observation of spontaneous bursts of cortical activity with scale-invariant sizes and durations, known as neuronal avalanches, in recordings of mesoscopic cortical signals. However, the existence of neuronal avalanches in spiking activity has been equivocal with studies reporting both its presence and absence. Here, we show that signs of criticality in spiking activity can change between synchronized and desynchronized cortical states. We analyzed the spontaneous activity in the primary visual cortex of the anesthetized cat and the awake monkey, and found that neuronal avalanches and thermodynamic indicators of criticality strongly depend on collective synchrony among neurons, LFP fluctuations, and behavioral state. We found that synchronized states are associated to criticality, large dynamical repertoire and prolonged epochs of eye closure, while desynchronized states are associated to sub-criticality, reduced dynamical repertoire, and eyes open conditions. Our results show that criticality in cortical dynamics is not stationary, but fluctuates during anesthesia and between different vigilance states. 
			<p>QC 20170704</p>","720270","Open Access","-0.6497","-0.2297","4","37m20, 65r20, 92b20","37m20, 65r20, 92b20","12","52","3","",""
"10.1371/journal.pone.0090578","dedup_wf_001::515d9661429ef1dc69ffd2d47a56fb3d","Computational Biology","Optogenetic Stimulation in a Computational Model of the Basal Ganglia Biases Action Selection and Reward Prediction Error","Berthet, Pierre","2014-01-01","KTH, Beräkningsbiologi, CB","publication","","","","Optogenetic stimulation of specific types of medium spiny neurons (MSNs) in the striatum has been shown to bias the selection of mice in a two choices task. This shift is dependent on the localisation and on the intensity of the stimulation but also on the recent reward history. We have implemented a way to simulate this increased activity produced by the optical flash in our computational model of the basal ganglia (BG). This abstract model features the direct and indirect pathways commonly described in biology, and a reward prediction pathway (RP). The framework is similar to Actor-Critic methods and to the ventral/ dorsal distinction in the striatum. We thus investigated the impact on the selection caused by an added stimulation in each of the three pathways. We were able to reproduce in our model the bias in action selection observed in mice. Our results also showed that biasing the reward prediction is sufficient to create a modification in the action selection. However, we had to increase the percentage of trials with stimulation relative to that in experiments in order to impact the selection. We found that increasing only the reward prediction had a different effect if the stimulation in RP was action dependent (only for a specific action) or not. We further looked at the evolution of the change in the weights depending on the stage of learning within a block. A bias in RP impacts the plasticity differently depending on that stage but also on the outcome. It remains to experimentally test how the dopaminergic neurons are affected by specific stimulations of neurons in the striatum and to relate data to predictions of our model. 
			<p>QC 20140428</p>","237955","Open Access","0.0946","0.6274","5","Basal ganglia, Action selection, Reward prediction error","Basal ganglia, Action selection, Reward prediction error",NA,NA,"1","",""
"10.1371/journal.pone.0134356","dedup_wf_001::af8a4486d28e64dc5201280069cff866","local;network;distributed","Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition.","Johannes Bill","","Public Library of Science (PLoS)","publication","","PLoS ONE","","During the last decade, Bayesian probability theory has emerged as a framework in cognitive science and neuroscience for describing perception, reasoning and learning of mammals. However, our understanding of how probabilistic computations could be organized in the brain, and how the observed connectivity structure of cortical microcircuits supports these calculations, is rudimentary at best. In this study, we investigate statistical inference and self-organized learning in a spatially extended spiking network model, that accommodates both local competitive and large-scale associative aspects of neural information processing, under a unified Bayesian account. Specifically, we show how the spiking dynamics of a recurrent network with lateral excitation and local inhibition in response to distributed spiking input, can be understood as sampling from a variational posterior distribution of a well-defined implicit probabilistic model. This interpretation further permits a rigorous analytical treatment of experience-dependent plasticity on the network level. Using machine learning theory, we derive update rules for neuron and synapse parameters which equate with Hebbian synaptic and homeostatic intrinsic plasticity rules in a neural implementation. In computer simulations, we demonstrate that the interplay of these plasticity rules leads to the emergence of probabilistic local experts that form distributed assemblies of similarly tuned cells communicating through lateral excitatory connections. The resulting sparse distributed spike code of a well-adapted network carries compressed information on salient input features combined with prior experience on correlations among them. Our theory predicts that the emergence of such efficient representations benefits from network architectures in which the range of local inhibition matches the spatial extent of pyramidal cells that share common afferent input.","I 753","Open Access","-0.2222","0.1938","8","Network model, Spiking neural network","Network model, Spiking neural network","2","47","1","",""
"10.3389/fnbeh.2012.00065","dedup_wf_001::28fee046684c6c1439722aa2c94eb130","basal ganglia","Action selection performance of a reconfigurable basal ganglia inspired model with Hebbian-Bayesian Go- NoGo connectivity","Berthet, Pierre","2012-01-01","FRONTIERS RES FOUND","publication","","FRONTIERS IN BEHAVIORAL NEUROSCIENCE","","Several studies have shown a strong involvement of the basal ganglia (BG) in action selection and dopamine dependent learning. The dopaminergic signal to striatum, the input stage of the BG, has been commonly described as coding a reward prediction error (RPE), i.e., the difference between the predicted and actual reward. The RPE has been hypothesized to be critical in the modulation of the synaptic plasticity in cortico-striatal synapses in the direct and indirect pathway. We developed an abstract computational model of the BG, with a dual pathway structure functionally corresponding to the direct and indirect pathways, and compared its behavior to biological data as well as other reinforcement learning models. The computations in our model are inspired by Bayesian inference, and the synaptic plasticity changes depend on a three factor Hebbian–Bayesian learning rule based on co-activation of pre- and post-synaptic units and on the value of the RPE. The model builds on a modified Actor-Critic architecture and implements the direct (Go) and the indirect (NoGo) pathway, as well as the reward prediction (RP) system, acting in a complementary fashion. We investigated the performance of the model system when different configurations of the Go, NoGo, and RP system were utilized, e.g., using only the Go, NoGo, or RP system, or combinations of those. Learning performance was investigated in several types of learning paradigms, such as learning-relearning, successive learning, stochastic learning, reversal learning and a two-choice task. The RPE and the activity of the model during learning were similar to monkey electrophysiological and behavioral data. Our results, however, show that there is not a unique best way to configure this BG model to handle well all the learning paradigms tested. We thus suggest that an agent might dynamically configure its action selection mode, possibly depending on task characteristics and also on how much time is available.","201716","Open Access","-0.0168","0.6122","5","Basal ganglia, Action selection, Reward prediction error","Basal ganglia, Action selection, Reward prediction error","1","63","8","",""
"10.3389/fncir.2014.00005","dedup_wf_001::b6794313d51b38d4b16e6def68aae38f","pattern rivalry","A spiking neural network model of self-organized pattern recognition in the early mammalian olfactory system","Kaplan, Bernhard A.","2014-02-07","Frontiers Media S.A.","publication","","Frontiers in Neural Circuits","","Olfactory sensory information passes through several processing stages before an odor percept emerges. The question how the olfactory system learns to create odor representations linking those different levels and how it learns to connect and discriminate between them is largely unresolved. We present a large-scale network model with single and multi-compartmental Hodgkin-Huxley type model neurons representing olfactory receptor neurons (ORNs) in the epithelium, periglomerular cells, mitral/tufted cells and granule cells in the olfactory bulb (OB), and three types of cortical cells in the piriform cortex (PC). Odor patterns are calculated based on affinities between ORNs and odor stimuli derived from physico-chemical descriptors of behaviorally relevant real-world odorants. The properties of ORNs were tuned to show saturated response curves with increasing concentration as seen in experiments. On the level of the OB we explored the possibility of using a fuzzy concentration interval code, which was implemented through dendro-dendritic inhibition leading to winner-take-all like dynamics between mitral/tufted cells belonging to the same glomerulus. The connectivity from mitral/tufted cells to PC neurons was self-organized from a mutual information measure and by using a competitive Hebbian-Bayesian learning algorithm based on the response patterns of mitral/tufted cells to different odors yielding a distributed feed-forward projection to the PC. The PC was implemented as a modular attractor network with a recurrent connectivity that was likewise organized through Hebbian-Bayesian learning. We demonstrate the functionality of the model in a one-sniff-learning and recognition task on a set of 50 odorants. Furthermore, we study its robustness against noise on the receptor level and its ability to perform concentration invariant odor recognition. Moreover, we investigate the pattern completion capabilities of the system and rivalry dynamics for odor mixtures. 
			<p>QC 20140313</p>","269921","Open Access","-0.4655","0.1701","8","Network model, Spiking neural network","Network model, Spiking neural network","2","49","12","",""
"10.3389/fncir.2016.00053","dedup_wf_001::86cbf8f1b712da97945e795c664b86ad","reward prediction error","Functional Relevance of Different Basal Ganglia Pathways Investigated in a Spiking Model with Reward Dependent Plasticity","Berthet, Pierre","2016-07-21","Frontiers Media S.A.","publication","","Frontiers in Neural Circuits","","The brain enables animals to behaviorally adapt in order to survive in a complex and dynamic environment, but how reward-oriented behaviors are achieved and computed by its underlying neural circuitry is an open question. To address this concern, we have developed a spiking model of the basal ganglia (BG) that learns to dis-inhibit the action leading to a reward despite ongoing changes in the reward schedule. The architecture of the network features the two pathways commonly described in BG, the direct (denoted D1) and the indirect (denoted D2) pathway, as well as a loop involving striatum and the dopaminergic system. The activity of these dopaminergic neurons conveys the reward prediction error (RPE), which determines the magnitude of synaptic plasticity within the different pathways. All plastic connections implement a versatile four-factor learning rule derived from Bayesian inference that depends upon pre- and post-synaptic activity, receptor type, and dopamine level. Synaptic weight updates occur in the D1 or D2 pathways depending on the sign of the RPE, and an efference copy informs upstream nuclei about the action selected. We demonstrate successful performance of the system in a multiple-choice learning task with a transiently changing reward schedule. We simulate lesioning of the various pathways and show that a condition without the D2 pathway fares worse than one without D1. Additionally, we simulate the degeneration observed in Parkinson's disease (PD) by decreasing the number of dopaminergic neurons during learning. The results suggest that the D1 pathway impairment in PD might have been overlooked. Furthermore, an analysis of the alterations in the synaptic weights shows that using the absolute reward value instead of the RPE leads to a larger change in D1.","237955","Open Access","-0.0541","0.497","5","Basal ganglia, Action selection, Reward prediction error","Basal ganglia, Action selection, Reward prediction error","3","28","2","",""
"10.3389/fncom.2010.00129","dedup_wf_001::8f7e443325bf55f83b7942e7aa4807f9","leaky integrate-and-fire neuron","Compensating inhomogeneities of neuromorphic VLSI devices via short-term synaptic plasticity","Bill, Johannes","2010-01-01","FRONTIERS RES FOUND","publication","","FRONTIERS IN COMPUTATIONAL NEUROSCIENCE","","
			Recent developments in neuromorphic hardware engineering make mixed-signal VLSI neural network models promising candidates for neuroscientific research tools and massively parallel computing devices, especially for tasks which exhaust the computing power of software simulations. Still, like all analog hardware systems, neuromorphic models suffer from a constricted configurability and production-related fluctuations of device characteristics. Since also future systems, involving ever-smaller structures, will inevitably exhibit such inhomogeneities on the unit level, self-regulation properties become a crucial requirement for their successful operation. By applying a cortically inspired self-adjusting network architecture, we show that the activity of generic spiking neural networks emulated on a neuromorphic hardware system can be kept within a biologically realistic firing regime and gain a remarkable robustness against transistor-level variations. As a first approach of this kind in engineering practice, the short-term synaptic depression and facilitation mechanisms implemented within an analog VLSI model of I&F neurons are functionally utilized for the purpose of network level stabilization. We present experimental data acquired both from the hardware model and from comparative software simulations which prove the applicability of the employed paradigm to neuromorphic VLSI devices.
			","237955","Open Access","0.3321","0.1959","3","General purpose, Neuromorphic hardware, Synaptic weight resolution","General purpose, Neuromorphic hardware, Synaptic weight resolution",NA,NA,"11","",""
"10.3389/fncom.2013.00112","dedup_wf_001::7f07ff20afa387345d2eb6116237d30a","Neurosciences","Anisotropic connectivity implements motion-based prediction in a spiking neural network","Kaplan, Bernhard A.","2013-01-01","FRONTIERS RESEARCH FOUNDATION","publication","","FRONTIERS IN COMPUTATIONAL NEUROSCIENCE","","Predictive coding hypothesizes that the brain explicitly infers upcoming sensory inputto establish a coherent representation of the world. Although it is becoming generallyaccepted, it is not clear on which level spiking neural networks may implementpredictive coding and what function their connectivity may have. We present a networkmodel of conductance-based integrate-and-fire neurons inspired by the architectureof retinotopic cortical areas that assumes predictive coding is implemented throughnetwork connectivity, namely in the connection delays and in selectiveness for the tuningproperties of source and target cells. We show that the applied connection pattern leadsto motion-based prediction in an experiment tracking a moving dot. In contrast to ourproposed model, a network with random or isotropic connectivity fails to predict the pathwhen the moving dot disappears. Furthermore, we show that a simple linear decodingapproach is sufficient to transform neuronal spiking activity into a probabilistic estimatefor reading out the target trajectory. 
			<p>QC 20140121</p>","269921","Open Access","0.0982","-0.3909","8","Network model, Spiking neural network","Network model, Spiking neural network","4","39","3","",""
"10.3389/fninf.2014.00076","dedup_wf_001::3a45d55d71aba12123ed1095016855c3","network simulator","Limits to high-speed simulations of spiking neural networks using general-purpose computers","Zenke, Friedemann","2014-01-01","Frontiers Research Foundation","publication","","","","To understand how the central nervous system performs computations using recurrent neuronal circuitry, simulations have become an indispensable tool for theoretical neuroscience. To study neuronal circuits and their ability to self-organize, increasing attention has been directed toward synaptic plasticity. In particular spike-timing-dependent plasticity (STDP) creates specific demands for simulations of spiking neural networks. On the one hand a high temporal resolution is required to capture the millisecond timescale of typical STDP windows. On the other hand network simulations have to evolve over hours up to days, to capture the timescale of long-term plasticity. To do this efficiently, fast simulation speed is the crucial ingredient rather than large neuron numbers. Using different medium-sized network models consisting of several thousands of neurons and off-the-shelf hardware, we compare the simulation speed of the simulators: Brian, NEST and Neuron as well as our own simulator Auryn. Our results show that real-time simulations of different plastic network models are possible in parallel simulations in which numerical precision is not a primary concern. Even so, the speed-up margin of parallelism is limited and boosting simulation speeds beyond one tenth of real-time is difficult. By profiling simulation code we show that the run times of typical plastic network simulations encounter a hard boundary. This limit is partly due to latencies in the inter-process communications and thus cannot be overcome by increased parallelism. Overall, these results show that to study plasticity in medium-sized spiking neural networks, adequate simulation tools are readily available which run efficiently on small clusters. However, to run simulations substantially faster than real-time, special hardware is a prerequisite.","268689","Open Access","0.1225","0.1195","3","General purpose, Neuromorphic hardware, Synaptic weight resolution","General purpose, Neuromorphic hardware, Synaptic weight resolution","1","74","14","",""
"10.3389/fnins.2011.00134","dedup_wf_001::923c5d8247c469257719e16b8c1c1b21","neuromimetic analog integrated circuits","Tunable Neuromimetic Integrated System for Emulating Cortical Neuron Models","Grassia, Filippo","2011-12-07","Frontiers Research Foundation","publication","","Frontiers in Neuroscience","","
			Nowadays, many software solutions are currently available for simulating neuron models. Less conventional than software-based systems, hardware-based solutions generally combine digital and analog forms of computation. In previous work, we designed several neuromimetic chips, including the Galway chip that we used for this paper. These silicon neurons are based on the Hodgkin–Huxley formalism and they are optimized for reproducing a large variety of neuron behaviors thanks to tunable parameters. Due to process variation and device mismatch in analog chips, we use a full-custom fitting method in voltage-clamp mode to tune our neuromimetic integrated circuits. By comparing them with experimental electrophysiological data of these cells, we show that the circuits can reproduce the main firing features of cortical cell types. In this paper, we present the experimental measurements of our system which mimic the four most prominent biological cells: fast spiking, regular spiking, intrinsically bursting, and low-threshold spiking neurons into analog neuromimetic integrated circuit dedicated to cortical neuron simulations. This hardware and software platform will allow to improve the hybrid technique, also called “dynamic-clamp,” that consists of connecting artificial and biological neurons to study the function of neuronal circuits.
			","237955","Open Access","0.2874","-0.0542","2","Bifurcation analysis, Contribution au développement, Cortical neuron models","Bifurcation analysis, Contribution au développement, Cortical neuron models",NA,NA,"9","",""
"10.3389/fnins.2012.00090","dedup_wf_001::a40fb31e30c9b032556106f21f94e9d0","synaptic weight resolution","Is a 4-Bit Synaptic Weight Resolution Enough? – Constraints on Enabling Spike-Timing Dependent Plasticity in Neuromorphic Hardware","Pfeil, Thomas","2012-07-01","Frontiers Research Foundation","publication","","Frontiers in Neuroscience","","
			Large-scale neuromorphic hardware systems typically bear the trade-off between detail level and required chip resources. Especially when implementing spike-timing dependent plasticity, reduction in resources leads to limitations as compared to floating point precision. By design, a natural modification that saves resources would be reducing synaptic weight resolution. In this study, we give an estimate for the impact of synaptic weight discretization on different levels, ranging from random walks of individual weights to computer simulations of spiking neural networks. The FACETS wafer-scale hardware system offers a 4-bit resolution of synaptic weights, which is shown to be sufficient within the scope of our network benchmark. Our findings indicate that increasing the resolution may not even be useful in light of further restrictions of customized mixed-signal synapses. In addition, variations due to production imperfections are investigated and shown to be uncritical in the context of the presented study. Our results represent a general framework for setting up and configuring hardware-constrained synapses. We suggest how weight discretization could be considered for other backends dedicated to large-scale simulations. Thus, our proposition of a good hardware verification practice may rise synergy effects between hardware developers and neuroscientists.
			","237955","Open Access","0.5826","0.3732","3","General purpose, Neuromorphic hardware, Synaptic weight resolution","General purpose, Neuromorphic hardware, Synaptic weight resolution","1","63","27","",""
